/* DO NOT MODIFY: auto-generated by migrate_libcxx_simd.sh.
   This file mirrors libcxx <experimental/simd>. By far it is still
   under code review, but all tests passed and performance is
   verified. We early-adopt this because (1) the API is published
   in the C++ parallelism v2 TS, (2) it takes long time to review,
   (3) we already have a lot of experience with the implementation.

   https://reviews.llvm.org/D44665 is the last patch under review.
   One can navigate all patches through the review system dependency
   management.  */

// -*- C++ -*-
//===--------------------------- __config ---------------------------------===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is dual licensed under the MIT and the University of Illinois Open
// Source Licenses. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//

#ifndef _DIMSUM_CONFIG
#define _DIMSUM_CONFIG

#if defined(_MSC_VER) && !defined(__clang__)
#  if !defined(_DIMSUM_HAS_NO_PRAGMA_SYSTEM_HEADER)
#    define _DIMSUM_HAS_NO_PRAGMA_SYSTEM_HEADER
#  endif
#endif

#ifndef _DIMSUM_HAS_NO_PRAGMA_SYSTEM_HEADER
#pragma GCC system_header
#endif

#ifdef __cplusplus

#ifdef __GNUC__
#  define _GNUC_VER (__GNUC__ * 100 + __GNUC_MINOR__)
// The _GNUC_VER_NEW macro better represents the new GCC versioning scheme
// introduced in GCC 5.0.
#  define _GNUC_VER_NEW (_GNUC_VER * 10 + __GNUC_PATCHLEVEL__)
#else
#  define _GNUC_VER 0
#  define _GNUC_VER_NEW 0
#endif

#define _DIMSUM_VERSION 7000

#ifndef _DIMSUM_ABI_VERSION
#define _DIMSUM_ABI_VERSION 1
#endif

#if defined(__ELF__)
#  define _DIMSUM_OBJECT_FORMAT_ELF   1
#elif defined(__MACH__)
#  define _DIMSUM_OBJECT_FORMAT_MACHO 1
#elif defined(_WIN32)
#  define _DIMSUM_OBJECT_FORMAT_COFF  1
#elif defined(__wasm__)
#  define _DIMSUM_OBJECT_FORMAT_WASM  1
#else
#  error Unknown object file format
#endif

#if defined(_DIMSUM_ABI_UNSTABLE) || _DIMSUM_ABI_VERSION >= 2
// Change short string representation so that string data starts at offset 0,
// improving its alignment in some cases.
#  define _DIMSUM_ABI_ALTERNATE_STRING_LAYOUT
// Fix deque iterator type in order to support incomplete types.
#  define _DIMSUM_ABI_INCOMPLETE_TYPES_IN_DEQUE
// Fix undefined behavior in how std::list stores its linked nodes.
#  define _DIMSUM_ABI_LIST_REMOVE_NODE_POINTER_UB
// Fix undefined behavior in  how __tree stores its end and parent nodes.
#  define _DIMSUM_ABI_TREE_REMOVE_NODE_POINTER_UB
// Fix undefined behavior in how __hash_table stores its pointer types.
#  define _DIMSUM_ABI_FIX_UNORDERED_NODE_POINTER_UB
#  define _DIMSUM_ABI_FORWARD_LIST_REMOVE_NODE_POINTER_UB
#  define _DIMSUM_ABI_FIX_UNORDERED_CONTAINER_SIZE_TYPE
// Don't use a nullptr_t simulation type in C++03 instead using C++11 nullptr
// provided under the alternate keyword __nullptr, which changes the mangling
// of nullptr_t. This option is ABI incompatible with GCC in C++03 mode.
#  define _DIMSUM_ABI_ALWAYS_USE_CXX11_NULLPTR
// Define the `pointer_safety` enum as a C++11 strongly typed enumeration
// instead of as a class simulating an enum. If this option is enabled
// `pointer_safety` and `get_pointer_safety()` will no longer be available
// in C++03.
#  define _DIMSUM_ABI_POINTER_SAFETY_ENUM_TYPE
// Define a key function for `bad_function_call` in the library, to centralize
// its vtable and typeinfo to libc++ rather than having all other libraries
// using that class define their own copies.
#  define _DIMSUM_ABI_BAD_FUNCTION_CALL_KEY_FUNCTION
// Enable optimized version of __do_get_(un)signed which avoids redundant copies.
#  define _DIMSUM_ABI_OPTIMIZED_LOCALE_NUM_GET
// Use the smallest possible integer type to represent the index of the variant.
// Previously libc++ used "unsigned int" exclusivly.
#  define _DIMSUM_ABI_VARIANT_INDEX_TYPE_OPTIMIZATION
#elif _DIMSUM_ABI_VERSION == 1
#  if !defined(_DIMSUM_OBJECT_FORMAT_COFF)
// Enable compiling copies of now inline methods into the dylib to support
// applications compiled against older libraries. This is unnecessary with
// COFF dllexport semantics, since dllexport forces a non-inline definition
// of inline functions to be emitted anyway. Our own non-inline copy would
// conflict with the dllexport-emitted copy, so we disable it.
#    define _DIMSUM_DEPRECATED_ABI_LEGACY_LIBRARY_DEFINITIONS_FOR_INLINE_FUNCTIONS
#  endif
// Feature macros for disabling pre ABI v1 features. All of these options
// are deprecated.
#  if defined(__FreeBSD__)
#    define _DIMSUM_DEPRECATED_ABI_DISABLE_PAIR_TRIVIAL_COPY_CTOR
#  endif
#endif

#ifdef _DIMSUM_TRIVIAL_PAIR_COPY_CTOR
#error "_DIMSUM_TRIVIAL_PAIR_COPY_CTOR" is no longer supported. \
       use _DIMSUM_DEPRECATED_ABI_DISABLE_PAIR_TRIVIAL_COPY_CTOR instead
#endif

#define _DIMSUM_CONCAT1(_DIMSUM_X,_DIMSUM_Y) _DIMSUM_X##_DIMSUM_Y
#define _DIMSUM_CONCAT(_DIMSUM_X,_DIMSUM_Y) _DIMSUM_CONCAT1(_DIMSUM_X,_DIMSUM_Y)

#define _DIMSUM_NAMESPACE _DIMSUM_CONCAT(__,_DIMSUM_ABI_VERSION)

#if __cplusplus < 201103L
#define _DIMSUM_CXX03_LANG
#endif

#ifndef __has_attribute
#define __has_attribute(__x) 0
#endif

#ifndef __has_builtin
#define __has_builtin(__x) 0
#endif

#ifndef __has_extension
#define __has_extension(__x) 0
#endif

#ifndef __has_feature
#define __has_feature(__x) 0
#endif

#ifndef __has_cpp_attribute
#define __has_cpp_attribute(__x) 0
#endif

// '__is_identifier' returns '0' if '__x' is a reserved identifier provided by
// the compiler and '1' otherwise.
#ifndef __is_identifier
#define __is_identifier(__x) 1
#endif

#ifndef __has_declspec_attribute
#define __has_declspec_attribute(__x) 0
#endif

#define __has_keyword(__x) !(__is_identifier(__x))

#ifdef __has_include
#  define __libcpp_has_include(__x) __has_include(__x)
#else
#  define __libcpp_has_include(__x) 0
#endif

#if defined(__clang__)
#  define _DIMSUM_COMPILER_CLANG
#  ifndef __apple_build_version__
#    define _DIMSUM_CLANG_VER (__clang_major__ * 100 + __clang_minor__)
#  endif
#elif defined(__GNUC__)
#  define _DIMSUM_COMPILER_GCC
#elif defined(_MSC_VER)
#  define _DIMSUM_COMPILER_MSVC
#elif defined(__IBMCPP__)
#  define _DIMSUM_COMPILER_IBM
#endif

#ifndef _DIMSUM_CLANG_VER
#define _DIMSUM_CLANG_VER 0
#endif

// FIXME: ABI detection should be done via compiler builtin macros. This
// is just a placeholder until Clang implements such macros. For now assume
// that Windows compilers pretending to be MSVC++ target the Microsoft ABI,
// and allow the user to explicitly specify the ABI to handle cases where this
// heuristic falls short.
#if defined(_DIMSUM_ABI_FORCE_ITANIUM) && defined(_DIMSUM_ABI_FORCE_MICROSOFT)
#  error "Only one of _DIMSUM_ABI_FORCE_ITANIUM and _DIMSUM_ABI_FORCE_MICROSOFT can be defined"
#elif defined(_DIMSUM_ABI_FORCE_ITANIUM)
#  define _DIMSUM_ABI_ITANIUM
#elif defined(_DIMSUM_ABI_FORCE_MICROSOFT)
#  define _DIMSUM_ABI_MICROSOFT
#else
#  if defined(_WIN32) && defined(_MSC_VER)
#    define _DIMSUM_ABI_MICROSOFT
#  else
#    define _DIMSUM_ABI_ITANIUM
#  endif
#endif

// Need to detect which libc we're using if we're on Linux.
#if defined(__linux__)
#  include <features.h>
#  if defined(__GLIBC_PREREQ)
#    define _DIMSUM_GLIBC_PREREQ(a, b) __GLIBC_PREREQ(a, b)
#  else
#    define _DIMSUM_GLIBC_PREREQ(a, b) 0
#  endif // defined(__GLIBC_PREREQ)
#endif // defined(__linux__)

#ifdef __LITTLE_ENDIAN__
#  if __LITTLE_ENDIAN__
#    define _DIMSUM_LITTLE_ENDIAN
#  endif  // __LITTLE_ENDIAN__
#endif  // __LITTLE_ENDIAN__

#ifdef __BIG_ENDIAN__
#  if __BIG_ENDIAN__
#    define _DIMSUM_BIG_ENDIAN
#  endif  // __BIG_ENDIAN__
#endif  // __BIG_ENDIAN__

#ifdef __BYTE_ORDER__
#  if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__
#    define _DIMSUM_LITTLE_ENDIAN
#  elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
#    define _DIMSUM_BIG_ENDIAN
#  endif // __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
#endif // __BYTE_ORDER__

#ifdef __FreeBSD__
#  include <sys/endian.h>
#  if _BYTE_ORDER == _LITTLE_ENDIAN
#    define _DIMSUM_LITTLE_ENDIAN
#  else  // _BYTE_ORDER == _LITTLE_ENDIAN
#    define _DIMSUM_BIG_ENDIAN
#  endif  // _BYTE_ORDER == _LITTLE_ENDIAN
#  ifndef __LONG_LONG_SUPPORTED
#    define _DIMSUM_HAS_NO_LONG_LONG
#  endif  // __LONG_LONG_SUPPORTED
#endif  // __FreeBSD__

#ifdef __NetBSD__
#  include <sys/endian.h>
#  if _BYTE_ORDER == _LITTLE_ENDIAN
#    define _DIMSUM_LITTLE_ENDIAN
#  else  // _BYTE_ORDER == _LITTLE_ENDIAN
#    define _DIMSUM_BIG_ENDIAN
#  endif  // _BYTE_ORDER == _LITTLE_ENDIAN
#  define _DIMSUM_HAS_QUICK_EXIT
#endif  // __NetBSD__

#if defined(_WIN32)
#  define _DIMSUM_WIN32API
#  define _DIMSUM_LITTLE_ENDIAN
#  define _DIMSUM_SHORT_WCHAR   1
// Both MinGW and native MSVC provide a "MSVC"-like enviroment
#  define _DIMSUM_MSVCRT_LIKE
// If mingw not explicitly detected, assume using MS C runtime only if
// a MS compatibility version is specified.
#  if defined(_MSC_VER) && !defined(__MINGW32__)
#    define _DIMSUM_MSVCRT // Using Microsoft's C Runtime library
#  endif
#  if (defined(_M_AMD64) || defined(__x86_64__)) || (defined(_M_ARM) || defined(__arm__))
#    define _DIMSUM_HAS_BITSCAN64
#  endif
#  define _DIMSUM_HAS_OPEN_WITH_WCHAR
#  if defined(_DIMSUM_MSVCRT)
#    define _DIMSUM_HAS_QUICK_EXIT
#  endif

// Some CRT APIs are unavailable to store apps
#  if defined(WINAPI_FAMILY)
#    include <winapifamily.h>
#    if !WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_DESKTOP) &&                  \
        (!defined(WINAPI_PARTITION_SYSTEM) ||                                  \
         !WINAPI_FAMILY_PARTITION(WINAPI_PARTITION_SYSTEM))
#      define _DIMSUM_WINDOWS_STORE_APP
#    endif
#  endif
#endif // defined(_WIN32)

#ifdef __sun__
#  include <sys/isa_defs.h>
#  ifdef _LITTLE_ENDIAN
#    define _DIMSUM_LITTLE_ENDIAN
#  else
#    define _DIMSUM_BIG_ENDIAN
#  endif
#endif // __sun__

#if defined(__CloudABI__)
   // Certain architectures provide arc4random(). Prefer using
   // arc4random() over /dev/{u,}random to make it possible to obtain
   // random data even when using sandboxing mechanisms such as chroots,
   // Capsicum, etc.
#  define _DIMSUM_USING_ARC4_RANDOM
#elif defined(__Fuchsia__)
#  define _DIMSUM_USING_GETENTROPY
#elif defined(__native_client__)
   // NaCl's sandbox (which PNaCl also runs in) doesn't allow filesystem access,
   // including accesses to the special files under /dev. C++11's
   // std::random_device is instead exposed through a NaCl syscall.
#  define _DIMSUM_USING_NACL_RANDOM
#elif defined(_DIMSUM_WIN32API)
#  define _DIMSUM_USING_WIN32_RANDOM
#else
#  define _DIMSUM_USING_DEV_RANDOM
#endif

#if !defined(_DIMSUM_LITTLE_ENDIAN) && !defined(_DIMSUM_BIG_ENDIAN)
#  include <endian.h>
#  if __BYTE_ORDER == __LITTLE_ENDIAN
#    define _DIMSUM_LITTLE_ENDIAN
#  elif __BYTE_ORDER == __BIG_ENDIAN
#    define _DIMSUM_BIG_ENDIAN
#  else  // __BYTE_ORDER == __BIG_ENDIAN
#    error unable to determine endian
#  endif
#endif  // !defined(_DIMSUM_LITTLE_ENDIAN) && !defined(_DIMSUM_BIG_ENDIAN)

#if __has_attribute(__no_sanitize__) && !defined(_DIMSUM_COMPILER_GCC)
#  define _DIMSUM_NO_CFI __attribute__((__no_sanitize__("cfi")))
#else
#  define _DIMSUM_NO_CFI
#endif

#if defined(_DIMSUM_COMPILER_CLANG)

// _DIMSUM_ALTERNATE_STRING_LAYOUT is an old name for
// _DIMSUM_ABI_ALTERNATE_STRING_LAYOUT left here for backward compatibility.
#if (defined(__APPLE__) && !defined(__i386__) && !defined(__x86_64__) &&       \
     (!defined(__arm__) || __ARM_ARCH_7K__ >= 2)) ||                           \
    defined(_DIMSUM_ALTERNATE_STRING_LAYOUT)
#define _DIMSUM_ABI_ALTERNATE_STRING_LAYOUT
#endif

#if __has_feature(cxx_alignas)
#  define _ALIGNAS_TYPE(x) alignas(x)
#  define _ALIGNAS(x) alignas(x)
#else
#  define _ALIGNAS_TYPE(x) __attribute__((__aligned__(__alignof(x))))
#  define _ALIGNAS(x) __attribute__((__aligned__(x)))
#endif

#if __cplusplus < 201103L
typedef __char16_t char16_t;
typedef __char32_t char32_t;
#endif

#if !(__has_feature(cxx_exceptions)) && !defined(_DIMSUM_NO_EXCEPTIONS)
#define _DIMSUM_NO_EXCEPTIONS
#endif

#if !(__has_feature(cxx_rtti)) && !defined(_DIMSUM_NO_RTTI)
#define _DIMSUM_NO_RTTI
#endif

#if !(__has_feature(cxx_strong_enums))
#define _DIMSUM_HAS_NO_STRONG_ENUMS
#endif

#if !(__has_feature(cxx_decltype))
#define _DIMSUM_HAS_NO_DECLTYPE
#endif

#if __has_feature(cxx_attributes)
#  define _DIMSUM_NORETURN [[noreturn]]
#else
#  define _DIMSUM_NORETURN __attribute__ ((noreturn))
#endif

#if !(__has_feature(cxx_lambdas))
#define _DIMSUM_HAS_NO_LAMBDAS
#endif

#if !(__has_feature(cxx_nullptr))
#  if (__has_extension(cxx_nullptr) || __has_keyword(__nullptr)) && defined(_DIMSUM_ABI_ALWAYS_USE_CXX11_NULLPTR)
#    define nullptr __nullptr
#  else
#    define _DIMSUM_HAS_NO_NULLPTR
#  endif
#endif

#if !(__has_feature(cxx_rvalue_references))
#define _DIMSUM_HAS_NO_RVALUE_REFERENCES
#endif

#if !(__has_feature(cxx_auto_type))
#define _DIMSUM_HAS_NO_AUTO_TYPE
#endif

#if !(__has_feature(cxx_variadic_templates))
#define _DIMSUM_HAS_NO_VARIADICS
#endif

#if !(__has_feature(cxx_generalized_initializers))
#define _DIMSUM_HAS_NO_GENERALIZED_INITIALIZERS
#endif

#if __has_feature(is_base_of)
#define _DIMSUM_HAS_IS_BASE_OF
#endif

#if __has_feature(is_final)
#define _DIMSUM_HAS_IS_FINAL
#endif

// Objective-C++ features (opt-in)
#if __has_feature(objc_arc)
#define _DIMSUM_HAS_OBJC_ARC
#endif

#if __has_feature(objc_arc_weak)
#define _DIMSUM_HAS_OBJC_ARC_WEAK
#endif

#if !(__has_feature(cxx_constexpr))
#define _DIMSUM_HAS_NO_CONSTEXPR
#endif

#if !(__has_feature(cxx_relaxed_constexpr))
#define _DIMSUM_HAS_NO_CXX14_CONSTEXPR
#endif

#if !(__has_feature(cxx_variable_templates))
#define _DIMSUM_HAS_NO_VARIABLE_TEMPLATES
#endif

#if __ISO_C_VISIBLE >= 2011 || __cplusplus >= 201103L
#  if defined(__FreeBSD__)
#    define _DIMSUM_HAS_QUICK_EXIT
#    define _DIMSUM_HAS_C11_FEATURES
#  elif defined(__Fuchsia__)
#    define _DIMSUM_HAS_QUICK_EXIT
#    define _DIMSUM_HAS_C11_FEATURES
#  elif defined(__linux__)
#    if !defined(_DIMSUM_HAS_MUSL_LIBC)
#      if _DIMSUM_GLIBC_PREREQ(2, 15) || defined(__BIONIC__)
#        define _DIMSUM_HAS_QUICK_EXIT
#      endif
#      if _DIMSUM_GLIBC_PREREQ(2, 17)
#        define _DIMSUM_HAS_C11_FEATURES
#      endif
#    else // defined(_DIMSUM_HAS_MUSL_LIBC)
#      define _DIMSUM_HAS_QUICK_EXIT
#      define _DIMSUM_HAS_C11_FEATURES
#    endif
#  endif // __linux__
#endif

#if !(__has_feature(cxx_noexcept))
#define _DIMSUM_HAS_NO_NOEXCEPT
#endif

#if __has_feature(underlying_type)
#define _DIMSUM_UNDERLYING_TYPE(T) __underlying_type(T)
#endif

#if __has_feature(is_literal)
#define _DIMSUM_IS_LITERAL(T) __is_literal(T)
#endif

// Inline namespaces are available in Clang regardless of C++ dialect.
#define _DIMSUM_BEGIN_NAMESPACE_STD namespace std {inline namespace _DIMSUM_NAMESPACE {
#define _DIMSUM_END_NAMESPACE_STD  } }
#define _VSTD std::_DIMSUM_NAMESPACE

namespace std {
  inline namespace _DIMSUM_NAMESPACE {
  }
}

#if !defined(_DIMSUM_HAS_NO_ASAN) && !__has_feature(address_sanitizer)
#define _DIMSUM_HAS_NO_ASAN
#endif

// Allow for build-time disabling of unsigned integer sanitization
#if !defined(_DIMSUM_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK) && __has_attribute(no_sanitize)
#define _DIMSUM_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK __attribute__((__no_sanitize__("unsigned-integer-overflow")))
#endif

#if __has_builtin(__builtin_launder)
#define _DIMSUM_COMPILER_HAS_BUILTIN_LAUNDER
#endif

#if !__is_identifier(__has_unique_object_representations)
#define _DIMSUM_HAS_UNIQUE_OBJECT_REPRESENTATIONS
#endif

#elif defined(_DIMSUM_COMPILER_GCC)

#define _ALIGNAS(x) __attribute__((__aligned__(x)))
#define _ALIGNAS_TYPE(x) __attribute__((__aligned__(__alignof(x))))

#define _DIMSUM_NORETURN __attribute__((noreturn))

#if _GNUC_VER >= 407
#define _DIMSUM_UNDERLYING_TYPE(T) __underlying_type(T)
#define _DIMSUM_IS_LITERAL(T) __is_literal_type(T)
#define _DIMSUM_HAS_IS_FINAL
#endif

#if defined(__GNUC__) && _GNUC_VER >= 403
#define _DIMSUM_HAS_IS_BASE_OF
#endif

#if !__EXCEPTIONS
#define _DIMSUM_NO_EXCEPTIONS
#endif

// constexpr was added to GCC in 4.6.
#if _GNUC_VER < 406
#  define _DIMSUM_HAS_NO_CONSTEXPR
// Can only use constexpr in c++11 mode.
#elif !defined(__GXX_EXPERIMENTAL_CXX0X__) && __cplusplus < 201103L
#  define _DIMSUM_HAS_NO_CONSTEXPR
#endif

// Determine if GCC supports relaxed constexpr
#if !defined(__cpp_constexpr) || __cpp_constexpr < 201304L
#define _DIMSUM_HAS_NO_CXX14_CONSTEXPR
#endif

// GCC 5 will support variable templates
#if !defined(__cpp_variable_templates) || __cpp_variable_templates < 201304L
#define _DIMSUM_HAS_NO_VARIABLE_TEMPLATES
#endif

#ifndef __GXX_EXPERIMENTAL_CXX0X__

#define _DIMSUM_HAS_NO_DECLTYPE
#define _DIMSUM_HAS_NO_NULLPTR
#define _DIMSUM_HAS_NO_UNICODE_CHARS
#define _DIMSUM_HAS_NO_VARIADICS
#define _DIMSUM_HAS_NO_RVALUE_REFERENCES
#define _DIMSUM_HAS_NO_STRONG_ENUMS
#define _DIMSUM_HAS_NO_NOEXCEPT

#else  // __GXX_EXPERIMENTAL_CXX0X__

#if _GNUC_VER < 403
#define _DIMSUM_HAS_NO_RVALUE_REFERENCES
#endif


#if _GNUC_VER < 404
#define _DIMSUM_HAS_NO_DECLTYPE
#define _DIMSUM_HAS_NO_UNICODE_CHARS
#define _DIMSUM_HAS_NO_VARIADICS
#define _DIMSUM_HAS_NO_GENERALIZED_INITIALIZERS
#endif  // _GNUC_VER < 404

#if _GNUC_VER < 406
#define _DIMSUM_HAS_NO_NOEXCEPT
#define _DIMSUM_HAS_NO_NULLPTR
#endif

#endif  // __GXX_EXPERIMENTAL_CXX0X__

#define _DIMSUM_BEGIN_NAMESPACE_STD namespace std { inline namespace _DIMSUM_NAMESPACE {
#define _DIMSUM_END_NAMESPACE_STD  } }
#define _VSTD std::_DIMSUM_NAMESPACE

namespace std {
  inline namespace _DIMSUM_NAMESPACE {
  }
}

#if !defined(_DIMSUM_HAS_NO_ASAN) && !defined(__SANITIZE_ADDRESS__)
#define _DIMSUM_HAS_NO_ASAN
#endif

#if _GNUC_VER >= 700
#define _DIMSUM_COMPILER_HAS_BUILTIN_LAUNDER
#endif

#if _GNUC_VER >= 700
#define _DIMSUM_HAS_UNIQUE_OBJECT_REPRESENTATIONS
#endif

#elif defined(_DIMSUM_COMPILER_MSVC)

#define _DIMSUM_TOSTRING2(x) #x
#define _DIMSUM_TOSTRING(x) _DIMSUM_TOSTRING2(x)
#define _DIMSUM_WARNING(x) __pragma(message(__FILE__ "(" _DIMSUM_TOSTRING(__LINE__) ") : warning note: " x))

#if _MSC_VER < 1900
#error "MSVC versions prior to Visual Studio 2015 are not supported"
#endif

#define _DIMSUM_HAS_IS_BASE_OF
#define _DIMSUM_HAS_NO_CONSTEXPR
#define _DIMSUM_HAS_NO_CXX14_CONSTEXPR
#define _DIMSUM_HAS_NO_VARIABLE_TEMPLATES
#if _MSC_VER <= 1800
#define _DIMSUM_HAS_NO_UNICODE_CHARS
#endif
#define _DIMSUM_HAS_NO_NOEXCEPT
#define __alignof__ __alignof
#define _DIMSUM_NORETURN __declspec(noreturn)
#define _ALIGNAS(x) __declspec(align(x))
#define _ALIGNAS_TYPE(x) alignas(x)
#define _DIMSUM_HAS_NO_VARIADICS

#define _DIMSUM_BEGIN_NAMESPACE_STD namespace std {
#define _DIMSUM_END_NAMESPACE_STD  }
#define _VSTD std

namespace std {
}

#define _DIMSUM_WEAK

#define _DIMSUM_HAS_NO_ASAN

#elif defined(_DIMSUM_COMPILER_IBM)

#define _ALIGNAS(x) __attribute__((__aligned__(x)))
#define _ALIGNAS_TYPE(x) __attribute__((__aligned__(__alignof(x))))
#define _ATTRIBUTE(x) __attribute__((x))
#define _DIMSUM_NORETURN __attribute__((noreturn))

#define _DIMSUM_HAS_NO_GENERALIZED_INITIALIZERS
#define _DIMSUM_HAS_NO_NOEXCEPT
#define _DIMSUM_HAS_NO_NULLPTR
#define _DIMSUM_HAS_NO_UNICODE_CHARS
#define _DIMSUM_HAS_IS_BASE_OF
#define _DIMSUM_HAS_IS_FINAL
#define _DIMSUM_HAS_NO_VARIABLE_TEMPLATES

#if defined(_AIX)
#define __MULTILOCALE_API
#endif

#define _DIMSUM_BEGIN_NAMESPACE_STD namespace std {inline namespace _DIMSUM_NAMESPACE {
#define _DIMSUM_END_NAMESPACE_STD  } }
#define _VSTD std::_DIMSUM_NAMESPACE

namespace std {
  inline namespace _DIMSUM_NAMESPACE {
  }
}

#define _DIMSUM_HAS_NO_ASAN

#endif // _DIMSUM_COMPILER_[CLANG|GCC|MSVC|IBM]

#if defined(_DIMSUM_OBJECT_FORMAT_COFF)

#ifdef _DLL
#  define _DIMSUM_CRT_FUNC __declspec(dllimport)
#else
#  define _DIMSUM_CRT_FUNC
#endif

#if defined(_DIMSUM_DISABLE_VISIBILITY_ANNOTATIONS)
#  define _DIMSUM_DLL_VIS
#  define _DIMSUM_EXTERN_TEMPLATE_TYPE_VIS
#  define _DIMSUM_CLASS_TEMPLATE_INSTANTIATION_VIS
#  define _DIMSUM_OVERRIDABLE_FUNC_VIS
#elif defined(_DIMSUM_BUILDING_LIBRARY)
#  define _DIMSUM_DLL_VIS __declspec(dllexport)
#  define _DIMSUM_EXTERN_TEMPLATE_TYPE_VIS
#  define _DIMSUM_CLASS_TEMPLATE_INSTANTIATION_VIS _DIMSUM_DLL_VIS
#  define _DIMSUM_OVERRIDABLE_FUNC_VIS _DIMSUM_DLL_VIS
#else
#  define _DIMSUM_DLL_VIS __declspec(dllimport)
#  define _DIMSUM_EXTERN_TEMPLATE_TYPE_VIS _DIMSUM_DLL_VIS
#  define _DIMSUM_CLASS_TEMPLATE_INSTANTIATION_VIS
#  define _DIMSUM_OVERRIDABLE_FUNC_VIS
#endif

#define _DIMSUM_TYPE_VIS            _DIMSUM_DLL_VIS
#define _DIMSUM_FUNC_VIS            _DIMSUM_DLL_VIS
#define _DIMSUM_EXTERN_VIS          _DIMSUM_DLL_VIS
#define _DIMSUM_EXCEPTION_ABI       _DIMSUM_DLL_VIS
#define _DIMSUM_HIDDEN
#define _DIMSUM_METHOD_TEMPLATE_IMPLICIT_INSTANTIATION_VIS
#define _DIMSUM_TEMPLATE_VIS
#define _DIMSUM_ENUM_VIS

#if defined(_DIMSUM_COMPILER_MSVC)
#  define _DIMSUM_INLINE_VISIBILITY __forceinline
#  define _DIMSUM_ALWAYS_INLINE     __forceinline
#  define _DIMSUM_EXTERN_TEMPLATE_INLINE_VISIBILITY __forceinline
#else
#  define _DIMSUM_INLINE_VISIBILITY __attribute__ ((__always_inline__))
#  define _DIMSUM_ALWAYS_INLINE     __attribute__ ((__always_inline__))
#  define _DIMSUM_EXTERN_TEMPLATE_INLINE_VISIBILITY __attribute__ ((__always_inline__))
#endif

#endif // defined(_DIMSUM_OBJECT_FORMAT_COFF)

#ifndef _DIMSUM_HIDDEN
#  if !defined(_DIMSUM_DISABLE_VISIBILITY_ANNOTATIONS)
#    define _DIMSUM_HIDDEN __attribute__ ((__visibility__("hidden")))
#  else
#    define _DIMSUM_HIDDEN
#  endif
#endif

#ifndef _DIMSUM_METHOD_TEMPLATE_IMPLICIT_INSTANTIATION_VIS
#  if !defined(_DIMSUM_DISABLE_VISIBILITY_ANNOTATIONS)
// The inline should be removed once PR32114 is resolved
#    define _DIMSUM_METHOD_TEMPLATE_IMPLICIT_INSTANTIATION_VIS inline _DIMSUM_HIDDEN
#  else
#    define _DIMSUM_METHOD_TEMPLATE_IMPLICIT_INSTANTIATION_VIS
#  endif
#endif

#ifndef _DIMSUM_FUNC_VIS
#  if !defined(_DIMSUM_DISABLE_VISIBILITY_ANNOTATIONS)
#    define _DIMSUM_FUNC_VIS __attribute__ ((__visibility__("default")))
#  else
#    define _DIMSUM_FUNC_VIS
#  endif
#endif

#ifndef _DIMSUM_TYPE_VIS
#  if !defined(_DIMSUM_DISABLE_VISIBILITY_ANNOTATIONS)
#    define _DIMSUM_TYPE_VIS __attribute__ ((__visibility__("default")))
#  else
#    define _DIMSUM_TYPE_VIS
#  endif
#endif

#ifndef _DIMSUM_TEMPLATE_VIS
#  if !defined(_DIMSUM_DISABLE_VISIBILITY_ANNOTATIONS)
#    if __has_attribute(__type_visibility__)
#      define _DIMSUM_TEMPLATE_VIS __attribute__ ((__type_visibility__("default")))
#    else
#      define _DIMSUM_TEMPLATE_VIS __attribute__ ((__visibility__("default")))
#    endif
#  else
#    define _DIMSUM_TEMPLATE_VIS
#  endif
#endif

#ifndef _DIMSUM_EXTERN_VIS
#define _DIMSUM_EXTERN_VIS
#endif

#ifndef _DIMSUM_OVERRIDABLE_FUNC_VIS
#define _DIMSUM_OVERRIDABLE_FUNC_VIS _DIMSUM_FUNC_VIS
#endif

#ifndef _DIMSUM_EXCEPTION_ABI
#  if !defined(_DIMSUM_DISABLE_VISIBILITY_ANNOTATIONS)
#    define _DIMSUM_EXCEPTION_ABI __attribute__ ((__visibility__("default")))
#  else
#    define _DIMSUM_EXCEPTION_ABI
#  endif
#endif

#ifndef _DIMSUM_ENUM_VIS
#  if !defined(_DIMSUM_DISABLE_VISIBILITY_ANNOTATIONS) && __has_attribute(__type_visibility__)
#    define _DIMSUM_ENUM_VIS __attribute__ ((__type_visibility__("default")))
#  else
#    define _DIMSUM_ENUM_VIS
#  endif
#endif

#ifndef _DIMSUM_EXTERN_TEMPLATE_TYPE_VIS
#  if !defined(_DIMSUM_DISABLE_VISIBILITY_ANNOTATIONS) && __has_attribute(__type_visibility__)
#    define _DIMSUM_EXTERN_TEMPLATE_TYPE_VIS __attribute__ ((__visibility__("default")))
#  else
#    define _DIMSUM_EXTERN_TEMPLATE_TYPE_VIS
#  endif
#endif

#ifndef _DIMSUM_CLASS_TEMPLATE_INSTANTIATION_VIS
#define _DIMSUM_CLASS_TEMPLATE_INSTANTIATION_VIS
#endif

#ifndef _DIMSUM_INLINE_VISIBILITY
#  if !defined(_DIMSUM_DISABLE_VISIBILITY_ANNOTATIONS)
#    define _DIMSUM_INLINE_VISIBILITY __attribute__ ((__visibility__("hidden"), __always_inline__))
#  else
#    define _DIMSUM_INLINE_VISIBILITY __attribute__ ((__always_inline__))
#  endif
#endif

#ifndef _DIMSUM_ALWAYS_INLINE
#  if !defined(_DIMSUM_DISABLE_VISIBILITY_ANNOTATIONS)
#    define _DIMSUM_ALWAYS_INLINE  __attribute__ ((__visibility__("hidden"), __always_inline__))
#  else
#    define _DIMSUM_ALWAYS_INLINE  __attribute__ ((__always_inline__))
#  endif
#endif

#ifndef _DIMSUM_EXTERN_TEMPLATE_INLINE_VISIBILITY
#  if !defined(_DIMSUM_DISABLE_VISIBILITY_ANNOTATIONS)
#    define _DIMSUM_EXTERN_TEMPLATE_INLINE_VISIBILITY __attribute__((__visibility__("default"), __always_inline__))
#  else
#    define _DIMSUM_EXTERN_TEMPLATE_INLINE_VISIBILITY __attribute__((__always_inline__))
#  endif
#endif

#ifndef _DIMSUM_PREFERRED_OVERLOAD
#  if __has_attribute(__enable_if__)
#    define _DIMSUM_PREFERRED_OVERLOAD __attribute__ ((__enable_if__(true, "")))
#  endif
#endif

#ifndef _DIMSUM_HAS_NO_NOEXCEPT
#  define _NOEXCEPT noexcept
#  define _NOEXCEPT_(x) noexcept(x)
#else
#  define _NOEXCEPT throw()
#  define _NOEXCEPT_(x)
#endif

#if defined(_DIMSUM_DEBUG_USE_EXCEPTIONS)
#  if !defined(_DIMSUM_DEBUG)
#    error cannot use _DIMSUM_DEBUG_USE_EXCEPTIONS unless _DIMSUM_DEBUG is defined
#  endif
#  ifdef _DIMSUM_HAS_NO_NOEXCEPT
#    define _NOEXCEPT_DEBUG
#    define _NOEXCEPT_DEBUG_(x)
#  else
#    define _NOEXCEPT_DEBUG noexcept(false)
#    define _NOEXCEPT_DEBUG_(x) noexcept(false)
#  endif
#else
#  define _NOEXCEPT_DEBUG _NOEXCEPT
#  define _NOEXCEPT_DEBUG_(x) _NOEXCEPT_(x)
#endif

#ifdef _DIMSUM_HAS_NO_UNICODE_CHARS
typedef unsigned short char16_t;
typedef unsigned int   char32_t;
#endif  // _DIMSUM_HAS_NO_UNICODE_CHARS

#ifndef __SIZEOF_INT128__
#define _DIMSUM_HAS_NO_INT128
#endif

#ifdef _DIMSUM_CXX03_LANG
#  if __has_extension(c_static_assert)
#    define static_assert(__b, __m) _Static_assert(__b, __m)
#  else
extern "C++" {
template <bool> struct __static_assert_test;
template <> struct __static_assert_test<true> {};
template <unsigned> struct __static_assert_check {};
}
#    define static_assert(__b, __m) \
       typedef __static_assert_check<sizeof(__static_assert_test<(__b)>)> \
       _DIMSUM_CONCAT(__t, __LINE__)
#  endif // __has_extension(c_static_assert)
#endif  // _DIMSUM_CXX03_LANG

#ifdef _DIMSUM_HAS_NO_DECLTYPE
// GCC 4.6 provides __decltype in all standard modes.
#  if __has_keyword(__decltype) || _DIMSUM_CLANG_VER >= 304 || _GNUC_VER >= 406
#    define decltype(__x) __decltype(__x)
#  else
#    define decltype(__x) __typeof__(__x)
#  endif
#endif

#ifdef _DIMSUM_HAS_NO_CONSTEXPR
#  define _DIMSUM_CONSTEXPR
#else
#  define _DIMSUM_CONSTEXPR constexpr
#endif

#ifdef _DIMSUM_CXX03_LANG
#  define _DIMSUM_DEFAULT {}
#else
#  define _DIMSUM_DEFAULT = default;
#endif

#ifdef _DIMSUM_CXX03_LANG
#  define _DIMSUM_EQUAL_DELETE
#else
#  define _DIMSUM_EQUAL_DELETE = delete
#endif

#ifdef __GNUC__
#  define _NOALIAS __attribute__((__malloc__))
#else
#  define _NOALIAS
#endif

#if __has_feature(cxx_explicit_conversions) || defined(__IBMCPP__) || \
    (!defined(_DIMSUM_CXX03_LANG) && defined(__GNUC__)) // All supported GCC versions
#  define _DIMSUM_EXPLICIT explicit
#else
#  define _DIMSUM_EXPLICIT
#endif

#if !__has_builtin(__builtin_operator_new) || !__has_builtin(__builtin_operator_delete)
#define _DIMSUM_HAS_NO_BUILTIN_OPERATOR_NEW_DELETE
#endif

#ifdef _DIMSUM_HAS_NO_STRONG_ENUMS
#  define _DIMSUM_DECLARE_STRONG_ENUM(x) struct _DIMSUM_TYPE_VIS x { enum __lx
#  define _DIMSUM_DECLARE_STRONG_ENUM_EPILOG(x) \
     __lx __v_; \
     _DIMSUM_ALWAYS_INLINE x(__lx __v) : __v_(__v) {} \
     _DIMSUM_ALWAYS_INLINE explicit x(int __v) : __v_(static_cast<__lx>(__v)) {} \
     _DIMSUM_ALWAYS_INLINE operator int() const {return __v_;} \
     };
#else  // _DIMSUM_HAS_NO_STRONG_ENUMS
#  define _DIMSUM_DECLARE_STRONG_ENUM(x) enum class _DIMSUM_ENUM_VIS x
#  define _DIMSUM_DECLARE_STRONG_ENUM_EPILOG(x)
#endif  // _DIMSUM_HAS_NO_STRONG_ENUMS

#ifdef _DIMSUM_DEBUG
#  if _DIMSUM_DEBUG == 0
#    define _DIMSUM_DEBUG_LEVEL 1
#  elif _DIMSUM_DEBUG == 1
#    define _DIMSUM_DEBUG_LEVEL 2
#  else
#    error Supported values for _DIMSUM_DEBUG are 0 and 1
#  endif
#  if !defined(_DIMSUM_BUILDING_LIBRARY)
#    define _DIMSUM_EXTERN_TEMPLATE(...)
#  endif
#endif

#ifdef _DIMSUM_DISABLE_EXTERN_TEMPLATE
#define _DIMSUM_EXTERN_TEMPLATE(...)
#define _DIMSUM_EXTERN_TEMPLATE2(...)
#endif

#ifndef _DIMSUM_EXTERN_TEMPLATE
#define _DIMSUM_EXTERN_TEMPLATE(...) extern template __VA_ARGS__;
#endif

#ifndef _DIMSUM_EXTERN_TEMPLATE2
#define _DIMSUM_EXTERN_TEMPLATE2(...) extern template __VA_ARGS__;
#endif

#if defined(__APPLE__) && defined(__LP64__) && !defined(__x86_64__)
#define _DIMSUM_NONUNIQUE_RTTI_BIT (1ULL << 63)
#endif

#if defined(__APPLE__) || defined(__FreeBSD__) || defined(_DIMSUM_MSVCRT_LIKE) || \
    defined(__sun__) || defined(__NetBSD__) || defined(__CloudABI__)
#define _DIMSUM_LOCALE__L_EXTENSIONS 1
#endif

#if defined(__unix__) || (defined(__APPLE__) && defined(__MACH__))
// Most unix variants have catopen.  These are the specific ones that don't.
#  if !defined(__BIONIC__) && !defined(_NEWLIB_VERSION)
#    define _DIMSUM_HAS_CATOPEN 1
#  endif
#endif

#ifdef __FreeBSD__
#define _DECLARE_C99_LDBL_MATH 1
#endif

#if defined(__APPLE__)
#  if !defined(__MAC_OS_X_VERSION_MIN_REQUIRED) && \
      defined(__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__)
#    define __MAC_OS_X_VERSION_MIN_REQUIRED __ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__
#  endif
#  if defined(__MAC_OS_X_VERSION_MIN_REQUIRED)
#    if __MAC_OS_X_VERSION_MIN_REQUIRED < 1060
#      define _DIMSUM_HAS_NO_ALIGNED_ALLOCATION
#    endif
#  endif
#endif // defined(__APPLE__)


#if defined(__APPLE__) || defined(__FreeBSD__)
#define _DIMSUM_HAS_DEFAULTRUNELOCALE
#endif

#if defined(__APPLE__) || defined(__FreeBSD__) || defined(__sun__)
#define _DIMSUM_WCTYPE_IS_MASK
#endif

#ifndef _DIMSUM_STD_VER
#  if  __cplusplus <= 201103L
#    define _DIMSUM_STD_VER 11
#  elif __cplusplus <= 201402L
#    define _DIMSUM_STD_VER 14
#  elif __cplusplus <= 201703L
#    define _DIMSUM_STD_VER 17
#  else
#    define _DIMSUM_STD_VER 18  // current year, or date of c++2a ratification
#  endif
#endif  // _DIMSUM_STD_VER

#if _DIMSUM_STD_VER > 11
#  define _DIMSUM_DEPRECATED [[deprecated]]
#else
#  define _DIMSUM_DEPRECATED
#endif

#if _DIMSUM_STD_VER <= 11
#  define _DIMSUM_EXPLICIT_AFTER_CXX11
#  define _DIMSUM_DEPRECATED_AFTER_CXX11
#else
#  define _DIMSUM_EXPLICIT_AFTER_CXX11 explicit
#  define _DIMSUM_DEPRECATED_AFTER_CXX11 [[deprecated]]
#endif

#if _DIMSUM_STD_VER > 11 && !defined(_DIMSUM_HAS_NO_CXX14_CONSTEXPR)
#  define _DIMSUM_CONSTEXPR_AFTER_CXX11 constexpr
#else
#  define _DIMSUM_CONSTEXPR_AFTER_CXX11
#endif

#if _DIMSUM_STD_VER > 14 && !defined(_DIMSUM_HAS_NO_CXX14_CONSTEXPR)
#  define _DIMSUM_CONSTEXPR_AFTER_CXX14 constexpr
#else
#  define _DIMSUM_CONSTEXPR_AFTER_CXX14
#endif

#if _DIMSUM_STD_VER > 17 && !defined(_DIMSUM_HAS_NO_CXX14_CONSTEXPR)
#  define _DIMSUM_CONSTEXPR_AFTER_CXX17 constexpr
#else
#  define _DIMSUM_CONSTEXPR_AFTER_CXX17
#endif

#if __has_cpp_attribute(nodiscard) && _DIMSUM_STD_VER > 17 && !defined(_DIMSUM_DISABLE_NODISCARD_AFTER_CXX17)
#  define _DIMSUM_NODISCARD_AFTER_CXX17 [[nodiscard]]
#else
#  define _DIMSUM_NODISCARD_AFTER_CXX17
#endif

#if _DIMSUM_STD_VER > 14 && defined(__cpp_inline_variables) && (__cpp_inline_variables >= 201606L)
#  define _DIMSUM_INLINE_VAR inline
#else
#  define _DIMSUM_INLINE_VAR
#endif

#ifdef _DIMSUM_HAS_NO_RVALUE_REFERENCES
#  define _DIMSUM_EXPLICIT_MOVE(x) _VSTD::move(x)
#else
#  define _DIMSUM_EXPLICIT_MOVE(x) (x)
#endif

#ifndef _DIMSUM_HAS_NO_ASAN
_DIMSUM_FUNC_VIS extern "C" void __sanitizer_annotate_contiguous_container(
  const void *, const void *, const void *, const void *);
#endif

// Try to find out if RTTI is disabled.
// g++ and cl.exe have RTTI on by default and define a macro when it is.
// g++ only defines the macro in 4.3.2 and onwards.
#if !defined(_DIMSUM_NO_RTTI)
#  if defined(__GNUC__) && \
      ((__GNUC__ >= 5) || \
       (__GNUC__ == 4 && (__GNUC_MINOR__ >= 3 || __GNUC_PATCHLEVEL__ >= 2))) && \
      !defined(__GXX_RTTI)
#    define _DIMSUM_NO_RTTI
#  elif defined(_DIMSUM_COMPILER_MSVC) && !defined(_CPPRTTI)
#    define _DIMSUM_NO_RTTI
#  endif
#endif

#ifndef _DIMSUM_WEAK
#define _DIMSUM_WEAK __attribute__((__weak__))
#endif

// Thread API
#if !defined(_DIMSUM_HAS_NO_THREADS) && \
    !defined(_DIMSUM_HAS_THREAD_API_PTHREAD) && \
    !defined(_DIMSUM_HAS_THREAD_API_WIN32) && \
    !defined(_DIMSUM_HAS_THREAD_API_EXTERNAL)
#  if defined(__FreeBSD__) || \
      defined(__Fuchsia__) || \
      defined(__NetBSD__) || \
      defined(__linux__) || \
      defined(__APPLE__) || \
      defined(__CloudABI__) || \
      defined(__sun__) || \
      (defined(__MINGW32__) && __libcpp_has_include(<pthread.h>))
#    define _DIMSUM_HAS_THREAD_API_PTHREAD
#  elif defined(_DIMSUM_WIN32API)
#    define _DIMSUM_HAS_THREAD_API_WIN32
#  else
#    error "No thread API"
#  endif // _DIMSUM_HAS_THREAD_API
#endif // _DIMSUM_HAS_NO_THREADS

#if defined(_DIMSUM_HAS_NO_THREADS) && defined(_DIMSUM_HAS_THREAD_API_PTHREAD)
#error _DIMSUM_HAS_THREAD_API_PTHREAD may only be defined when \
       _DIMSUM_HAS_NO_THREADS is not defined.
#endif

#if defined(_DIMSUM_HAS_NO_THREADS) && defined(_DIMSUM_HAS_THREAD_API_EXTERNAL)
#error _DIMSUM_HAS_THREAD_API_EXTERNAL may not be defined when \
       _DIMSUM_HAS_NO_THREADS is defined.
#endif

#if defined(_DIMSUM_HAS_NO_MONOTONIC_CLOCK) && !defined(_DIMSUM_HAS_NO_THREADS)
#error _DIMSUM_HAS_NO_MONOTONIC_CLOCK may only be defined when \
       _DIMSUM_HAS_NO_THREADS is defined.
#endif

// Systems that use capability-based security (FreeBSD with Capsicum,
// Nuxi CloudABI) may only provide local filesystem access (using *at()).
// Functions like open(), rename(), unlink() and stat() should not be
// used, as they attempt to access the global filesystem namespace.
#ifdef __CloudABI__
#define _DIMSUM_HAS_NO_GLOBAL_FILESYSTEM_NAMESPACE
#endif

// CloudABI is intended for running networked services. Processes do not
// have standard input and output channels.
#ifdef __CloudABI__
#define _DIMSUM_HAS_NO_STDIN
#define _DIMSUM_HAS_NO_STDOUT
#endif

#if defined(__BIONIC__) || defined(__CloudABI__) ||                            \
    defined(__Fuchsia__) || defined(_DIMSUM_HAS_MUSL_LIBC)
#define _DIMSUM_PROVIDES_DEFAULT_RUNE_TABLE
#endif

// Thread-unsafe functions such as strtok() and localtime()
// are not available.
#ifdef __CloudABI__
#define _DIMSUM_HAS_NO_THREAD_UNSAFE_C_FUNCTIONS
#endif

#if __has_feature(cxx_atomic) || __has_extension(c_atomic) || __has_keyword(_Atomic)
#  define _DIMSUM_HAS_C_ATOMIC_IMP
#elif _GNUC_VER > 407
#  define _DIMSUM_HAS_GCC_ATOMIC_IMP
#endif

#if (!defined(_DIMSUM_HAS_C_ATOMIC_IMP) && !defined(_DIMSUM_HAS_GCC_ATOMIC_IMP)) \
     || defined(_DIMSUM_HAS_NO_THREADS)
#define _DIMSUM_HAS_NO_ATOMIC_HEADER
#endif

#ifndef _DIMSUM_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK
#define _DIMSUM_DISABLE_UBSAN_UNSIGNED_INTEGER_CHECK
#endif

#if defined(_DIMSUM_ENABLE_THREAD_SAFETY_ANNOTATIONS)
#  if defined(__clang__) && __has_attribute(acquire_capability)
// Work around the attribute handling in clang.  When both __declspec and
// __attribute__ are present, the processing goes awry preventing the definition
// of the types.
#    if !defined(_DIMSUM_OBJECT_FORMAT_COFF)
#      define _DIMSUM_HAS_THREAD_SAFETY_ANNOTATIONS
#    endif
#  endif
#endif

#if __has_attribute(require_constant_initialization)
#  define _DIMSUM_SAFE_STATIC __attribute__((__require_constant_initialization__))
#else
#  define _DIMSUM_SAFE_STATIC
#endif

#if !__has_builtin(__builtin_addressof) && _GNUC_VER < 700
#define _DIMSUM_HAS_NO_BUILTIN_ADDRESSOF
#endif

#if !defined(_DIMSUM_HAS_NO_OFF_T_FUNCTIONS)
#  if defined(_DIMSUM_MSVCRT) || defined(_NEWLIB_VERSION)
#    define _DIMSUM_HAS_NO_OFF_T_FUNCTIONS
#  endif
#endif

#if __has_attribute(diagnose_if) && !defined(_DIMSUM_DISABLE_ADDITIONAL_DIAGNOSTICS)
#  define _DIMSUM_DIAGNOSE_WARNING(...) \
     __attribute__((diagnose_if(__VA_ARGS__, "warning")))
#  define _DIMSUM_DIAGNOSE_ERROR(...) \
     __attribute__((diagnose_if(__VA_ARGS__, "error")))
#else
#  define _DIMSUM_DIAGNOSE_WARNING(...)
#  define _DIMSUM_DIAGNOSE_ERROR(...)
#endif

#if __has_attribute(fallthough) || _GNUC_VER >= 700
// Use a function like macro to imply that it must be followed by a semicolon
#  define _DIMSUM_FALLTHROUGH() __attribute__((__fallthrough__))
#else
#  define _DIMSUM_FALLTHROUGH() ((void)0)
#endif

#if defined(_DIMSUM_ABI_MICROSOFT) && \
    (defined(_DIMSUM_COMPILER_MSVC) || __has_declspec_attribute(empty_bases))
#  define _DIMSUM_DECLSPEC_EMPTY_BASES __declspec(empty_bases)
#else
#  define _DIMSUM_DECLSPEC_EMPTY_BASES
#endif

#if defined(_DIMSUM_ENABLE_CXX17_REMOVED_FEATURES)
#define _DIMSUM_ENABLE_CXX17_REMOVED_AUTO_PTR
#define _DIMSUM_ENABLE_CXX17_REMOVED_UNEXPECTED_FUNCTIONS
#define _DIMSUM_ENABLE_CXX17_REMOVED_RANDOM_SHUFFLE
#define _DIMSUM_ENABLE_CXX17_REMOVED_BINDERS
#endif // _DIMSUM_ENABLE_CXX17_REMOVED_FEATURES

#if !defined(__cpp_deduction_guides) || __cpp_deduction_guides < 201611
#define _DIMSUM_HAS_NO_DEDUCTION_GUIDES
#endif

#if !__has_keyword(__is_aggregate) && (_GNUC_VER_NEW < 7001)
#define _DIMSUM_HAS_NO_IS_AGGREGATE
#endif

#if !defined(__cpp_coroutines) || __cpp_coroutines < 201703L
#define _DIMSUM_HAS_NO_COROUTINES
#endif

// FIXME: Correct this macro when either (A) a feature test macro for the
// spaceship operator is provided, or (B) a compiler provides a complete
// implementation.
#define _DIMSUM_HAS_NO_SPACESHIP_OPERATOR

// Decide whether to use availability macros.
#if !defined(_DIMSUM_BUILDING_LIBRARY) &&                                      \
    !defined(_DIMSUM_DISABLE_AVAILABILITY) &&                                  \
    __has_feature(attribute_availability_with_strict) &&                       \
    __has_feature(attribute_availability_in_templates)
#  ifdef __APPLE__
#    define _DIMSUM_USE_AVAILABILITY_APPLE
#  endif
#endif

// Define availability macros.
#if defined(_DIMSUM_USE_AVAILABILITY_APPLE)
#  define _DIMSUM_AVAILABILITY_SHARED_MUTEX                                    \
     __attribute__((availability(macosx,strict,introduced=10.12)))             \
     __attribute__((availability(ios,strict,introduced=10.0)))                 \
     __attribute__((availability(tvos,strict,introduced=10.0)))                \
     __attribute__((availability(watchos,strict,introduced=3.0)))
#  define _DIMSUM_AVAILABILITY_BAD_OPTIONAL_ACCESS __attribute__((unavailable))
#  define _DIMSUM_AVAILABILITY_BAD_ARRAY_LENGTH __attribute__((unavailable))
#  define _DIMSUM_AVAILABILITY_BAD_ANY_CAST __attribute__((unavailable))
#  define _DIMSUM_AVAILABILITY_UNCAUGHT_EXCEPTIONS                             \
     __attribute__((availability(macosx,strict,introduced=10.12)))             \
     __attribute__((availability(ios,strict,introduced=10.0)))                 \
     __attribute__((availability(tvos,strict,introduced=10.0)))                \
     __attribute__((availability(watchos,strict,introduced=3.0)))
#  define _DIMSUM_AVAILABILITY_SIZED_NEW_DELETE                                \
     __attribute__((availability(macosx,strict,introduced=10.12)))             \
     __attribute__((availability(ios,strict,introduced=10.0)))                 \
     __attribute__((availability(tvos,strict,introduced=10.0)))                \
     __attribute__((availability(watchos,strict,introduced=3.0)))
#  define _DIMSUM_AVAILABILITY_FUTURE_ERROR                                    \
     __attribute__((availability(ios,strict,introduced=6.0)))
#  define _DIMSUM_AVAILABILITY_TYPEINFO_VTABLE                                 \
     __attribute__((availability(macosx,strict,introduced=10.9)))              \
     __attribute__((availability(ios,strict,introduced=7.0)))
#  define _DIMSUM_AVAILABILITY_LOCALE_CATEGORY                                 \
     __attribute__((availability(macosx,strict,introduced=10.9)))              \
     __attribute__((availability(ios,strict,introduced=7.0)))
#  define _DIMSUM_AVAILABILITY_ATOMIC_SHARED_PTR                               \
     __attribute__((availability(macosx,strict,introduced=10.9)))              \
     __attribute__((availability(ios,strict,introduced=7.0)))
#else
#  define _DIMSUM_AVAILABILITY_SHARED_MUTEX
#  define _DIMSUM_AVAILABILITY_BAD_OPTIONAL_ACCESS
#  define _DIMSUM_AVAILABILITY_BAD_ARRAY_LENGTH
#  define _DIMSUM_AVAILABILITY_BAD_ANY_CAST
#  define _DIMSUM_AVAILABILITY_UNCAUGHT_EXCEPTIONS
#  define _DIMSUM_AVAILABILITY_SIZED_NEW_DELETE
#  define _DIMSUM_AVAILABILITY_FUTURE_ERROR
#  define _DIMSUM_AVAILABILITY_TYPEINFO_VTABLE
#  define _DIMSUM_AVAILABILITY_LOCALE_CATEGORY
#  define _DIMSUM_AVAILABILITY_ATOMIC_SHARED_PTR
#endif

// Define availability that depends on _DIMSUM_NO_EXCEPTIONS.
#ifdef _DIMSUM_NO_EXCEPTIONS
#  define _DIMSUM_AVAILABILITY_DYNARRAY
#  define _DIMSUM_AVAILABILITY_FUTURE
#  define _DIMSUM_AVAILABILITY_THROW_BAD_ANY_CAST
#else
#  define _DIMSUM_AVAILABILITY_DYNARRAY _DIMSUM_AVAILABILITY_BAD_ARRAY_LENGTH
#  define _DIMSUM_AVAILABILITY_FUTURE _DIMSUM_AVAILABILITY_FUTURE_ERROR
#  define _DIMSUM_AVAILABILITY_THROW_BAD_ANY_CAST                              \
     _DIMSUM_AVAILABILITY_BAD_ANY_CAST
#endif

// Availability of stream API in the dylib got dropped and re-added.  The
// extern template should effectively be available at:
//    availability(macosx,introduced=10.9)
//    availability(ios,introduced=7.0)
#if defined(_DIMSUM_USE_AVAILABILITY_APPLE) &&                                 \
    ((defined(__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__) &&                \
      __ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__ < 1090) ||                 \
     (defined(__ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__) &&               \
      __ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__ < 70000))
#define _DIMSUM_AVAILABILITY_NO_STREAMS_EXTERN_TEMPLATE
#endif

#if defined(_DIMSUM_COMPILER_IBM)
#define _DIMSUM_HAS_NO_PRAGMA_PUSH_POP_MACRO
#endif

#if defined(_DIMSUM_HAS_NO_PRAGMA_PUSH_POP_MACRO)
#  define _DIMSUM_PUSH_MACROS
#  define _DIMSUM_POP_MACROS
#else
  // Don't warn about macro conflicts when we can restore them at the
  // end of the header.
#  ifndef _DIMSUM_DISABLE_MACRO_CONFLICT_WARNINGS
#    define _DIMSUM_DISABLE_MACRO_CONFLICT_WARNINGS
#  endif
#  if defined(_DIMSUM_COMPILER_MSVC)
#    define _DIMSUM_PUSH_MACROS    \
       __pragma(push_macro("min")) \
       __pragma(push_macro("max"))
#    define _DIMSUM_POP_MACROS     \
       __pragma(pop_macro("min"))  \
       __pragma(pop_macro("max"))
#  else
#    define _DIMSUM_PUSH_MACROS        \
       _Pragma("push_macro(\"min\")")  \
       _Pragma("push_macro(\"max\")")
#    define _DIMSUM_POP_MACROS         \
       _Pragma("pop_macro(\"min\")")   \
       _Pragma("pop_macro(\"max\")")
#  endif
#endif // defined(_DIMSUM_HAS_NO_PRAGMA_PUSH_POP_MACRO)

#ifndef _DIMSUM_NO_AUTO_LINK
#  if defined(_DIMSUM_ABI_MICROSOFT) && !defined(_DIMSUM_BUILDING_LIBRARY)
#    if defined(_DLL)
#      pragma comment(lib, "c++.lib")
#    else
#      pragma comment(lib, "libc++.lib")
#    endif
#  endif // defined(_DIMSUM_ABI_MICROSOFT) && !defined(_DIMSUM_BUILDING_LIBRARY)
#endif // _DIMSUM_NO_AUTO_LINK

#if !defined(_DIMSUM_COMPILER_CLANG) && !defined(_DIMSUM_COMPILER_GCC)
#define _DIMSUM_HAS_NO_VECTOR_EXTENSION
#endif // defined(_DIMSUM_COMPILER_CLANG) || defined(_DIMSUM_COMPILER_GCC)

#endif // __cplusplus

#endif // _DIMSUM_CONFIG
// -*- C++ -*-
//===--------------------------- __config ---------------------------------===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is dual licensed under the MIT and the University of Illinois Open
// Source Licenses. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//

#ifndef _DIMSUM_EXPERIMENTAL_CONFIG
#define _DIMSUM_EXPERIMENTAL_CONFIG



#if !defined(_DIMSUM_HAS_NO_PRAGMA_SYSTEM_HEADER)
#pragma GCC system_header
#endif

#define _DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL namespace std { namespace experimental {
#define _DIMSUM_END_NAMESPACE_EXPERIMENTAL  } }
#define _VSTD_EXPERIMENTAL std::experimental

#define _DIMSUM_BEGIN_NAMESPACE_LFTS _DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL inline namespace fundamentals_v1 {
#define _DIMSUM_END_NAMESPACE_LFTS  } } }
#define _VSTD_LFTS _VSTD_EXPERIMENTAL::fundamentals_v1

#define _DIMSUM_BEGIN_NAMESPACE_LFTS_V2 _DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL inline namespace fundamentals_v2 {
#define _DIMSUM_END_NAMESPACE_LFTS_V2  } } }
#define _VSTD_LFTS_V2 _VSTD_EXPERIMENTAL::fundamentals_v2

#define _DIMSUM_BEGIN_NAMESPACE_LFTS_PMR _DIMSUM_BEGIN_NAMESPACE_LFTS namespace pmr {
#define _DIMSUM_END_NAMESPACE_LFTS_PMR _DIMSUM_END_NAMESPACE_LFTS }
#define _VSTD_LFTS_PMR _VSTD_LFTS::pmr

#define _DIMSUM_BEGIN_NAMESPACE_CHRONO_LFTS _DIMSUM_BEGIN_NAMESPACE_STD        \
  namespace chrono { namespace experimental { inline namespace fundamentals_v1 {
#define _DIMSUM_END_NAMESPACE_CHRONO_LFTS _DIMSUM_END_NAMESPACE_STD } } }

#define _DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL_FILESYSTEM \
    _DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL namespace filesystem { \
    inline namespace v1 {

#define _DIMSUM_END_NAMESPACE_EXPERIMENTAL_FILESYSTEM \
    } } _DIMSUM_END_NAMESPACE_EXPERIMENTAL

#define _DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL_COROUTINES \
  _DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL inline namespace coroutines_v1 {

#define _DIMSUM_END_NAMESPACE_EXPERIMENTAL_COROUTINES \
  } _DIMSUM_END_NAMESPACE_EXPERIMENTAL

#define _VSTD_CORO _VSTD_EXPERIMENTAL::coroutines_v1

#define _VSTD_FS ::std::experimental::filesystem::v1

#define _DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL_SIMD \
    _DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL inline namespace parallelism_v2 {

#define _DIMSUM_END_NAMESPACE_EXPERIMENTAL_SIMD \
    } _DIMSUM_END_NAMESPACE_EXPERIMENTAL

#define _DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL_SIMD_ABI \
    _DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL_SIMD namespace simd_abi {

#define _DIMSUM_END_NAMESPACE_EXPERIMENTAL_SIMD_ABI \
    } _DIMSUM_END_NAMESPACE_EXPERIMENTAL_SIMD

#if defined(__SSE2__)
#define _DIMSUM_MICROARCH_SSE2
#if defined(__AVX__)
#define _DIMSUM_MICROARCH_AVX
#endif
#elif defined(__ALTIVEC__) || defined(__VSX__)
#define _DIMSUM_MICROARCH_ALTIVEC
#elif defined(__ARM_NEON)
#define _DIMSUM_MICROARCH_NEON
#endif

#endif
// -*- C++ -*-
//===------------------------------- simd ---------------------------------===//
//
//                     The LLVM Compiler Infrastructure
//
// This file is dual licensed under the MIT and the University of Illinois Open
// Source Licenses. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//
#ifndef _LIBCPP_EXPERIMENTAL_SIMD
#define _LIBCPP_EXPERIMENTAL_SIMD

/*
    experimental/simd synopsis

namespace std::experimental {

inline namespace parallelism_v2 {

namespace simd_abi {
  struct scalar {};
  template <int N> struct fixed_size {};
  template <typename T> inline constexpr int max_fixed_size = implementation-defined;
  template <typename T> using compatible = implementation-defined;
  template <typename T> using native = implementation-defined;
  template <class T, size_t N> struct deduce { using type = see below; };
  template <class T, size_t N> using deduce_t = typename deduce<T, N>::type;
}

struct element_aligned_tag {};
struct vector_aligned_tag {};
template <size_t> struct overaligned_tag {};

inline constexpr element_aligned_tag element_aligned{};
inline constexpr vector_aligned_tag vector_aligned{};
template <size_t N> inline constexpr overaligned_tag<N> overaligned{};

// traits [simd.traits]
template <class T> struct is_abi_tag;
template <class T> inline constexpr bool is_abi_tag_v = is_abi_tag<T>::value;

template <class T> struct is_simd;
template <class T> inline constexpr bool is_simd_v = is_simd<T>::value;

template <class T> struct is_simd_mask;
template <class T> inline constexpr bool is_simd_mask_v = is_simd_mask<T>::value;

template <class T> struct is_simd_flag_type;
template <class T> inline constexpr bool is_simd_flag_type_v = is_simd_flag_type<T>::value;

template <class T, class Abi = simd_abi::compatible<T>> struct simd_size;
template <class T, class Abi = simd_abi::compatible<T>>
inline constexpr size_t simd_size_v = simd_size<T, Abi>::value;

template <class T, class U = typename T::value_type> struct memory_alignment;
template <class T, class U = typename T::value_type>
inline constexpr size_t memory_alignment_v = memory_alignment<T, U>::value;

// class template simd [simd.class]
template <class T, class Abi = simd_abi::compatible<T>> class simd;
template <class T> using native_simd = simd<T, simd_abi::native<T>>;
template <class T, int N> using fixed_size_simd = simd<T, simd_abi::fixed_size<N>>;

// class template simd_mask [simd.mask.class]
template <class T, class Abi = simd_abi::compatible<T>> class simd_mask;
template <class T> using native_simd_mask = simd_mask<T, simd_abi::native<T>>;
template <class T, int N> using fixed_size_simd_mask = simd_mask<T, simd_abi::fixed_size<N>>;

// casts [simd.casts]
template <class T, class U, class Abi> see below simd_cast(const simd<U, Abi>&);
template <class T, class U, class Abi> see below static_simd_cast(const simd<U, Abi>&);

template <class T, class Abi>
fixed_size_simd<T, simd_size_v<T, Abi>> to_fixed_size(const simd<T, Abi>&) noexcept;
template <class T, class Abi>
fixed_size_simd_mask<T, simd_size_v<T, Abi>> to_fixed_size(const simd_mask<T, Abi>&) noexcept;
template <class T, int N> native_simd<T> to_native(const fixed_size_simd<T, N>&) noexcept;
template <class T, int N> native_simd_mask<T> to_native(const fixed_size_simd_mask<T, N>&) noexcept;
template <class T, int N> simd<T> to_compatible(const fixed_size_simd<T, N>&) noexcept;
template <class T, int N> simd_mask<T> to_compatible(const fixed_size_simd_mask<T, N>&) noexcept;

template <size_t... Sizes, class T, class Abi>
tuple<simd<T, simd_abi::deduce_t<T, Sizes>>...> split(const simd<T, Abi>&);
template <size_t... Sizes, class T, class Abi>
tuple<simd_mask<T, simd_abi::deduce_t<T, Sizes>>...> split(const simd_mask<T, Abi>&);
template <class V, class Abi>
array<V, simd_size_v<typename V::value_type, Abi> / V::size()> split(
const simd<typename V::value_type, Abi>&);
template <class V, class Abi>
array<V, simd_size_v<typename V::value_type, Abi> / V::size()> split(
const simd_mask<typename V::value_type, Abi>&);

template <class T, class... Abis>
simd<T, simd_abi::deduce_t<T, (simd_size_v<T, Abis> + ...)>> concat(const simd<T, Abis>&...);
template <class T, class... Abis>
simd_mask<T, simd_abi::deduce_t<T, (simd_size_v<T, Abis> + ...)>> concat(const simd_mask<T, Abis>&...);

// reductions [simd.mask.reductions]
template <class T, class Abi> bool all_of(const simd_mask<T, Abi>&) noexcept;
template <class T, class Abi> bool any_of(const simd_mask<T, Abi>&) noexcept;
template <class T, class Abi> bool none_of(const simd_mask<T, Abi>&) noexcept;
template <class T, class Abi> bool some_of(const simd_mask<T, Abi>&) noexcept;
template <class T, class Abi> int popcount(const simd_mask<T, Abi>&) noexcept;
template <class T, class Abi> int find_first_set(const simd_mask<T, Abi>&);
template <class T, class Abi> int find_last_set(const simd_mask<T, Abi>&);

bool all_of(see below) noexcept;
bool any_of(see below) noexcept;
bool none_of(see below) noexcept;
bool some_of(see below) noexcept;
int popcount(see below) noexcept;
int find_first_set(see below) noexcept;
int find_last_set(see below) noexcept;

// masked assignment [simd.whereexpr]
template <class M, class T> class const_where_expression;
template <class M, class T> class where_expression;

// masked assignment [simd.mask.where]
template <class T> struct nodeduce { using type = T; }; // exposition only
template <class T> using nodeduce_t = typename nodeduce<T>::type; // exposition only

template <class T, class Abi>
where_expression<simd_mask<T, Abi>, simd<T, Abi>>
where(const typename simd<T, Abi>::mask_type&, simd<T, Abi>&) noexcept;
template <class T, class Abi>
const_where_expression<simd_mask<T, Abi>, simd<T, Abi>>
where(const typename simd<T, Abi>::mask_type&, const simd<T, Abi>&) noexcept;

template <class T, class Abi>
where_expression<simd_mask<T, Abi>, simd_mask<T, Abi>>
where(const nodeduce_t<simd_mask<T, Abi>>&, simd_mask<T, Abi>&) noexcept;
template <class T, class Abi>
const_where_expression<simd_mask<T, Abi>, simd_mask<T, Abi>>
where(const nodeduce_t<simd_mask<T, Abi>>&, const simd_mask<T, Abi>&) noexcept;

template <class T> where_expression<bool, T> where(see below k, T& d) noexcept;
template <class T>
const_where_expression<bool, T> where(see below k, const T& d) noexcept;

// reductions [simd.reductions]
template <class T, class Abi, class BinaryOperation = plus<>>
T reduce(const simd<T, Abi>&, BinaryOperation = {});

template <class M, class V, class BinaryOperation>
typename V::value_type reduce(const const_where_expression<M, V>& x,
typename V::value_type identity_element, BinaryOperation binary_op);
template <class M, class V>
typename V::value_type reduce(const const_where_expression<M, V>& x, plus<> binary_op = {});
template <class M, class V>
typename V::value_type reduce(const const_where_expression<M, V>& x, multiplies<> binary_op);
template <class M, class V>
typename V::value_type reduce(const const_where_expression<M, V>& x, bit_and<> binary_op);
template <class M, class V>
typename V::value_type reduce(const const_where_expression<M, V>& x, bit_or<> binary_op);
template <class M, class V>
typename V::value_type reduce(const const_where_expression<M, V>& x, bit_xor<> binary_op);

template <class T, class Abi> T hmin(const simd<T, Abi>&);
template <class M, class V> typename V::value_type hmin(const const_where_expression<M, V>&);
template <class T, class Abi> T hmax(const simd<T, Abi>&);
template <class M, class V> typename V::value_type hmax(const const_where_expression<M, V>&);

// algorithms [simd.alg]
template <class T, class Abi> simd<T, Abi> min(const simd<T, Abi>&, const simd<T, Abi>&) noexcept;
template <class T, class Abi> simd<T, Abi> max(const simd<T, Abi>&, const simd<T, Abi>&) noexcept;
template <class T, class Abi>
pair<simd<T, Abi>, simd<T, Abi>> minmax(const simd<T, Abi>&, const simd<T, Abi>&) noexcept;
template <class T, class Abi>
simd<T, Abi> clamp(const simd<T, Abi>& v, const simd<T, Abi>& lo, const simd<T, Abi>& hi);

// [simd.whereexpr]
template <class M, class T>
class const_where_expression {
  const M mask; // exposition only
  T& data; // exposition only

public:
  const_where_expression(const const_where_expression&) = delete;
  const_where_expression& operator=(const const_where_expression&) = delete;

  T operator-() const &&;
  T operator+() const &&;
  T operator~() const &&;

  template <class U, class Flags> void copy_to(U* mem, Flags f) const &&;
};

template <class M, class T>
class where_expression : public const_where_expression<M, T> {
public:
  template <class U> void operator=(U&& x) &&;
  template <class U> void operator+=(U&& x) &&;
  template <class U> void operator-=(U&& x) &&;
  template <class U> void operator*=(U&& x) &&;
  template <class U> void operator/=(U&& x) &&;
  template <class U> void operator%=(U&& x) &&;
  template <class U> void operator&=(U&& x) &&;
  template <class U> void operator|=(U&& x) &&;
  template <class U> void operator^=(U&& x) &&;
  template <class U> void operator<<=(U&& x) &&;
  template <class U> void operator>>=(U&& x) &&;
  void operator++() &&;
  void operator++(int) &&;
  void operator--() &&;
  void operator--(int) &&;

  template <class U, class Flags> void copy_from(const U* mem, Flags) &&;
};

// [simd.class]
template <class T, class Abi>
class simd {
public:
  using value_type = T;
  using reference = see below;
  using mask_type = simd_mask<T, Abi>;
  using abi_type = Abi;

  static constexpr size_t size() noexcept;

  simd() = default;

  // implicit type conversion constructor
  template <class U> simd(const simd<U, simd_abi::fixed_size<size()>>&);

  // implicit broadcast constructor (see below for constraints)
  template <class U> simd(U&& value);

  // generator constructor (see below for constraints)
  template <class G> explicit simd(G&& gen);

  // load constructor
  template <class U, class Flags> simd(const U* mem, Flags f);

  // loads [simd.load]
  template <class U, class Flags> void copy_from(const U* mem, Flags f);

  // stores [simd.store]
  template <class U, class Flags> void copy_to(U* mem, Flags f) const;

  // scalar access [simd.subscr]
  reference operator[](size_t);
  value_type operator[](size_t) const;

  // unary operators [simd.unary]
  simd& operator++();
  simd operator++(int);
  simd& operator--();
  simd operator--(int);
  mask_type operator!() const;
  simd operator~() const; // see below
  simd operator+() const;
  simd operator-() const;

  // binary operators [simd.binary]
  friend simd operator+ (const simd&, const simd&);
  friend simd operator- (const simd&, const simd&);
  friend simd operator* (const simd&, const simd&);
  friend simd operator/ (const simd&, const simd&);
  friend simd operator% (const simd&, const simd&);
  friend simd operator& (const simd&, const simd&);
  friend simd operator| (const simd&, const simd&);
  friend simd operator^ (const simd&, const simd&);
  friend simd operator<<(const simd&, const simd&);
  friend simd operator>>(const simd&, const simd&);
  friend simd operator<<(const simd&, int);
  friend simd operator>>(const simd&, int);

  // compound assignment [simd.cassign]
  friend simd& operator+= (simd&, const simd&);
  friend simd& operator-= (simd&, const simd&);
  friend simd& operator*= (simd&, const simd&);
  friend simd& operator/= (simd&, const simd&);
  friend simd& operator%= (simd&, const simd&);
  friend simd& operator&= (simd&, const simd&);
  friend simd& operator|= (simd&, const simd&);
  friend simd& operator^= (simd&, const simd&);
  friend simd& operator<<=(simd&, const simd&);
  friend simd& operator>>=(simd&, const simd&);
  friend simd& operator<<=(simd&, int);
  friend simd& operator>>=(simd&, int);

  // compares [simd.comparison]
  friend mask_type operator==(const simd&, const simd&);
  friend mask_type operator!=(const simd&, const simd&);
  friend mask_type operator>=(const simd&, const simd&);
  friend mask_type operator<=(const simd&, const simd&);
  friend mask_type operator> (const simd&, const simd&);
  friend mask_type operator< (const simd&, const simd&);
};

class reference // exposition only
{
public:
  reference() = delete;
  reference(const reference &) = delete;

  operator value_type() const noexcept;

  template <class U> reference operator=(U&& x) &&;

  template <class U> reference operator+=(U&& x) &&;
  template <class U> reference operator-=(U&& x) &&;
  template <class U> reference operator*=(U&& x) &&;
  template <class U> reference operator/=(U&& x) &&;
  template <class U> reference operator%=(U&& x) &&;
  template <class U> reference operator|=(U&& x) &&;
  template <class U> reference operator&=(U&& x) &&;
  template <class U> reference operator^=(U&& x) &&;
  template <class U> reference operator<<=(U&& x) &&;
  template <class U> reference operator>>=(U&& x) &&;

  reference operator++() &&;
  value_type operator++(int) &&;
  reference operator--() &&;
  value_type operator--(int) &&;

  friend void swap(reference&& a, reference&& b) noexcept;
  friend void swap(value_type& a, reference&& b) noexcept;
  friend void swap(reference&& a, value_type& b) noexcept;
};

// [simd.mask.class]
template <class T, class Abi>
class simd_mask {
public:
  using value_type = bool;
  using reference = see below;
  using simd_type = simd<T, Abi>;
  using abi_type = Abi;

  static constexpr size_t size() noexcept;

  simd_mask() = default;

  // broadcast constructor
  explicit simd_mask(value_type) noexcept;

  // implicit type conversion constructor
  template <class U> simd_mask(const simd_mask<U, simd_abi::fixed_size<size()>>&) noexcept;

  // load constructor
  template <class Flags> simd_mask(const value_type* mem, Flags);

  // loads [simd.mask.copy]
  template <class Flags> void copy_from(const value_type* mem, Flags);
  template <class Flags> void copy_to(value_type* mem, Flags) const;

  // scalar access [simd.mask.subscr]
  reference operator[](size_t);
  value_type operator[](size_t) const;

  // unary operators [simd.mask.unary]
  simd_mask operator!() const noexcept;

  // simd_mask binary operators [simd.mask.binary]
  friend simd_mask operator&&(const simd_mask&, const simd_mask&) noexcept;
  friend simd_mask operator||(const simd_mask&, const simd_mask&) noexcept;
  friend simd_mask operator& (const simd_mask&, const simd_mask&) noexcept;
  friend simd_mask operator| (const simd_mask&, const simd_mask&) noexcept;
  friend simd_mask operator^ (const simd_mask&, const simd_mask&) noexcept;

  // simd_mask compound assignment [simd.mask.cassign]
  friend simd_mask& operator&=(simd_mask&, const simd_mask&) noexcept;
  friend simd_mask& operator|=(simd_mask&, const simd_mask&) noexcept;
  friend simd_mask& operator^=(simd_mask&, const simd_mask&) noexcept;

  // simd_mask compares [simd.mask.comparison]
  friend simd_mask operator==(const simd_mask&, const simd_mask&) noexcept;
  friend simd_mask operator!=(const simd_mask&, const simd_mask&) noexcept;
};

} // parallelism_v2
} // std::experimental

*/

#include "index_sequence.h"
#include <algorithm>
#include <array>
#include <cstddef>
#include <cstring>
#include <functional>
#include <limits>

#if defined(_DIMSUM_MICROARCH_SSE2)
#include <emmintrin.h>
#if defined(_DIMSUM_MICROARCH_AVX)
#include <immintrin.h>
#endif
#elif defined(_DIMSUM_MICROARCH_ALTIVEC)
#include <altivec.h>
#elif defined(_DIMSUM_MICROARCH_NEON)
#include <arm_neon.h>
#endif

#if !defined(_DIMSUM_HAS_NO_PRAGMA_SYSTEM_HEADER)
#pragma GCC system_header
#endif

#if !defined(_DIMSUM_COMPILER_CLANG)
#define _DIMSUM_UNROLL
#else
// See LLVM PR/36359 for context of this workaround.
#define _DIMSUM_UNROLL _Pragma("unroll")
#endif

_DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL_SIMD

enum class _StorageKind {
  _Scalar,
  _Array,
  _VecExt,
};

template <_StorageKind __kind, int _Np>
struct __simd_abi {};

template <class _Derived>
struct __simd_storage_base {
  static _Derived __neg(const _Derived& __a) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, -__a.__get(__i));
    }
    return __v;
  }

  static _Derived __add(const _Derived& __a, const _Derived& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) + __b.__get(__i));
    }
    return __v;
  }

  static _Derived __sub(const _Derived& __a, const _Derived& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) - __b.__get(__i));
    }
    return __v;
  }

  static _Derived __mul(const _Derived& __a, const _Derived& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) * __b.__get(__i));
    }
    return __v;
  }

  static _Derived __div(const _Derived& __a, const _Derived& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) / __b.__get(__i));
    }
    return __v;
  }

  static _Derived __mod(const _Derived& __a, const _Derived& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) % __b.__get(__i));
    }
    return __v;
  }

  static _Derived __and(const _Derived& __a, const _Derived& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) & __b.__get(__i));
    }
    return __v;
  }

  static _Derived __or(const _Derived& __a, const _Derived& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) | __b.__get(__i));
    }
    return __v;
  }

  static _Derived __not(const _Derived& __a) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, ~__a.__get(__i));
    }
    return __v;
  }

  static _Derived __xor(const _Derived& __a, const _Derived& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) ^ __b.__get(__i));
    }
    return __v;
  }

  static _Derived __shl(const _Derived& __a, const _Derived& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) << __b.__get(__i));
    }
    return __v;
  }

  static _Derived __shr(const _Derived& __a, const _Derived& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) >> __b.__get(__i));
    }
    return __v;
  }

  template <class _InputSimd>
  static _Derived __cmp_eq(const _InputSimd& __a, const _InputSimd& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) == __b.__get(__i) ? -1 : 0);
    }
    return __v;
  }

  template <class _InputSimd>
  static _Derived __cmp_ne(const _InputSimd& __a, const _InputSimd& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) != __b.__get(__i) ? -1 : 0);
    }
    return __v;
  }

  template <class _InputSimd>
  static _Derived __cmp_le(const _InputSimd& __a, const _InputSimd& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) <= __b.__get(__i) ? -1 : 0);
    }
    return __v;
  }

  template <class _InputSimd>
  static _Derived __cmp_ge(const _InputSimd& __a, const _InputSimd& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) >= __b.__get(__i) ? -1 : 0);
    }
    return __v;
  }

  template <class _InputSimd>
  static _Derived __cmp_lt(const _InputSimd& __a, const _InputSimd& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) < __b.__get(__i) ? -1 : 0);
    }
    return __v;
  }

  template <class _InputSimd>
  static _Derived __cmp_gt(const _InputSimd& __a, const _InputSimd& __b) {
    _Derived __v;
    for (size_t __i = 0; __i < _Derived::__size(); __i++) {
      __v.__set(__i, __a.__get(__i) > __b.__get(__i) ? -1 : 0);
    }
    return __v;
  }
};

template <class _Tp, class _Abi>
class __simd_storage {};

template <class _Tp, int __num_element>
class __simd_storage<_Tp, __simd_abi<_StorageKind::_Array, __num_element>>
    : public __simd_storage_base<__simd_storage<
          _Tp, __simd_abi<_StorageKind::_Array, __num_element>>> {
  std::array<_Tp, __num_element> __storage_;

  template <class, class>
  friend struct simd;

  template <class, class>
  friend struct simd_mask;

public:
  using __raw_type = std::array<_Tp, __num_element>;

  static constexpr size_t __size() { return __num_element; }

  __simd_storage() = default;
  void __assign(__raw_type __raw) { __storage_ = __raw; }
  __raw_type __raw() const { return __storage_; }
  _Tp __get(size_t __index) const { return __storage_[__index]; };
  void __set(size_t __index, _Tp __val) { __storage_[__index] = __val; }
};

template <class _Tp>
class __simd_storage<_Tp, __simd_abi<_StorageKind::_Scalar, 1>>
    : public __simd_storage_base<
          __simd_storage<_Tp, __simd_abi<_StorageKind::_Scalar, 1>>> {
  _Tp __storage_;

  template <class, class>
  friend struct simd;

  template <class, class>
  friend struct simd_mask;

public:
  using __raw_type = _Tp;

  static constexpr size_t __size() { return 1; }

  __simd_storage() = default;
  void __assign(__raw_type __raw) { __storage_ = __raw; }
  __raw_type __raw() const { return __storage_; }
  _Tp __get(size_t __index) const { return (&__storage_)[__index]; };
  void __set(size_t __index, _Tp __val) { (&__storage_)[__index] = __val; }
};

#ifndef _DIMSUM_HAS_NO_VECTOR_EXTENSION

constexpr size_t __floor_pow_of_2(size_t __val) {
  return ((__val - 1) & __val) == 0 ? __val
                                    : __floor_pow_of_2((__val - 1) & __val);
}

constexpr size_t __ceil_pow_of_2(size_t __val) {
  return __val == 1 ? 1 : __floor_pow_of_2(__val - 1) << 1;
}

template <class _Tp, size_t __bytes>
struct __vec_ext_traits {
#if !defined(_DIMSUM_COMPILER_CLANG)
  typedef _Tp type __attribute__((vector_size(__ceil_pow_of_2(__bytes))));
#endif
};

#if defined(_DIMSUM_COMPILER_CLANG)
#define _SPECIALIZE_VEC_EXT(_TYPE, _NUM_ELEMENT)                               \
  template <>                                                                  \
  struct __vec_ext_traits<_TYPE, sizeof(_TYPE) * _NUM_ELEMENT> {               \
    using type =                                                               \
        _TYPE __attribute__((vector_size(sizeof(_TYPE) * _NUM_ELEMENT)));      \
  }

#define _SPECIALIZE_VEC_EXT_FOR_SIZES(_TYPE)                                   \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x01);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x02);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x03);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x04);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x05);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x06);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x07);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x08);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x09);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x0a);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x0b);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x0c);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x0d);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x0e);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x0f);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x10);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x11);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x12);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x13);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x14);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x15);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x16);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x17);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x18);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x19);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x1a);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x1b);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x1c);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x1d);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x1e);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x1f);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x20);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x21);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x22);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x23);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x24);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x25);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x26);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x27);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x28);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x29);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x2a);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x2b);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x2c);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x2d);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x2e);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x2f);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x30);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x31);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x32);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x33);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x34);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x35);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x36);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x37);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x38);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x39);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x3a);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x3b);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x3c);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x3d);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x3e);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x3f);                                            \
  _SPECIALIZE_VEC_EXT(_TYPE, 0x40);

_SPECIALIZE_VEC_EXT_FOR_SIZES(char);
_SPECIALIZE_VEC_EXT_FOR_SIZES(char16_t);
_SPECIALIZE_VEC_EXT_FOR_SIZES(char32_t);
_SPECIALIZE_VEC_EXT_FOR_SIZES(wchar_t);
_SPECIALIZE_VEC_EXT_FOR_SIZES(signed char);
_SPECIALIZE_VEC_EXT_FOR_SIZES(signed short);
_SPECIALIZE_VEC_EXT_FOR_SIZES(signed int);
_SPECIALIZE_VEC_EXT_FOR_SIZES(signed long);
_SPECIALIZE_VEC_EXT_FOR_SIZES(signed long long);
_SPECIALIZE_VEC_EXT_FOR_SIZES(unsigned char);
_SPECIALIZE_VEC_EXT_FOR_SIZES(unsigned short);
_SPECIALIZE_VEC_EXT_FOR_SIZES(unsigned int);
_SPECIALIZE_VEC_EXT_FOR_SIZES(unsigned long);
_SPECIALIZE_VEC_EXT_FOR_SIZES(unsigned long long);
_SPECIALIZE_VEC_EXT_FOR_SIZES(float);
_SPECIALIZE_VEC_EXT_FOR_SIZES(double);
_SPECIALIZE_VEC_EXT_FOR_SIZES(long double);

#undef _SPECIALIZE_VEC_EXT_FOR_SIZES
#undef _SPECIALIZE_VEC_EXT
#endif

template <class _Tp, int __num_element>
class __simd_storage<_Tp, __simd_abi<_StorageKind::_VecExt, __num_element>>
    : public __simd_storage_base<__simd_storage<
          _Tp, __simd_abi<_StorageKind::_VecExt, __num_element>>> {
  using _StorageType =
      typename __vec_ext_traits<_Tp, sizeof(_Tp) * __num_element>::type;

  _StorageType __storage_;

  template <class, class>
  friend struct simd;

  template <class, class>
  friend struct simd_mask;

  __simd_storage(_StorageType __s) : __storage_(__s) {}

public:
  using __raw_type = _StorageType;

  static constexpr size_t __size() { return __num_element; }

  __simd_storage() = default;
  void __assign(__raw_type __raw) { __storage_ = __raw; }
  __raw_type __raw() const { return __storage_; }

  _Tp __get(size_t __index) const { return __storage_[__index]; };
  void __set(size_t __index, _Tp __val) { __storage_[__index] = __val; }

  static __simd_storage __neg(const __simd_storage& __a) {
    return __simd_storage(-__a.__storage_);
  }

  static __simd_storage __add(const __simd_storage& __a,
                              const __simd_storage& __b) {
    return __simd_storage(__a.__storage_ + __b.__storage_);
  }

  static __simd_storage __sub(const __simd_storage& __a,
                              const __simd_storage& __b) {
    return __simd_storage(__a.__storage_ - __b.__storage_);
  }

  static __simd_storage __mul(const __simd_storage& __a,
                              const __simd_storage& __b) {
    return __simd_storage(__a.__storage_ * __b.__storage_);
  }

  static __simd_storage __div(const __simd_storage& __a,
                              const __simd_storage& __b) {
    return __simd_storage(__a.__storage_ / __b.__storage_);
  }

  static __simd_storage __mod(const __simd_storage& __a,
                              const __simd_storage& __b) {
    return __simd_storage(__a.__storage_ % __b.__storage_);
  }

  static __simd_storage __and(const __simd_storage& __a,
                              const __simd_storage& __b) {
    return __simd_storage(__a.__storage_ & __b.__storage_);
  }

  static __simd_storage __or(const __simd_storage& __a,
                             const __simd_storage& __b) {
    return __simd_storage(__a.__storage_ | __b.__storage_);
  }

  static __simd_storage __xor(const __simd_storage& __a,
                              const __simd_storage& __b) {
    return __simd_storage(__a.__storage_ ^ __b.__storage_);
  }

  static __simd_storage __not(const __simd_storage& __a) {
    return __simd_storage(~__a.__storage_);
  }

  static __simd_storage __shl(const __simd_storage& __a,
                              const __simd_storage& __b) {
    return __simd_storage(__a.__storage_ << __b.__storage_);
  }

  static __simd_storage __shr(const __simd_storage& __a,
                              const __simd_storage& __b) {
    return __simd_storage(__a.__storage_ >> __b.__storage_);
  }

  template <class _InputSimd>
  static __simd_storage __cmp_eq(const _InputSimd& __a, const _InputSimd& __b) {
    return __simd_storage(__a.__raw() == __b.__raw());
  }

  template <class _InputSimd>
  static __simd_storage __cmp_ne(const _InputSimd& __a, const _InputSimd& __b) {
    return __simd_storage(__a.__raw() != __b.__raw());
  }

  template <class _InputSimd>
  static __simd_storage __cmp_le(const _InputSimd& __a, const _InputSimd& __b) {
    return __simd_storage(__a.__raw() <= __b.__raw());
  }

  template <class _InputSimd>
  static __simd_storage __cmp_ge(const _InputSimd& __a, const _InputSimd& __b) {
    return __simd_storage(__a.__raw() >= __b.__raw());
  }

  template <class _InputSimd>
  static __simd_storage __cmp_lt(const _InputSimd& __a, const _InputSimd& __b) {
    return __simd_storage(__a.__raw() < __b.__raw());
  }

  template <class _InputSimd>
  static __simd_storage __cmp_gt(const _InputSimd& __a, const _InputSimd& __b) {
    return __simd_storage(__a.__raw() > __b.__raw());
  }
};

#endif // _DIMSUM_HAS_NO_VECTOR_EXTENSION

template <class _Tp, size_t __bytes>
struct __native_type_traits {
  using type = void;
};

#if defined(_DIMSUM_MICROARCH_SSE2)

template <class _Tp>
struct __native_type_traits<_Tp, 16> {
  static_assert(16 % sizeof(_Tp) == 0, "");
  using type = __m128i;
};

template <>
struct __native_type_traits<float, 16> {
  using type = __m128;
};

template <>
struct __native_type_traits<double, 16> {
  using type = __m128d;
};

#if defined(_DIMSUM_MICROARCH_AVX)

template <class _Tp>
struct __native_type_traits<_Tp, 32> {
  static_assert(32 % sizeof(_Tp) == 0, "");
  using type = __m256i;
};

template <>
struct __native_type_traits<float, 32> {
  using type = __m256;
};

template <>
struct __native_type_traits<double, 32> {
  using type = __m256d;
};
#endif // _DIMSUM_MICROARCH_AVX
#elif defined(_DIMSUM_MICROARCH_ALTIVEC)

#define _SPECIALIZE_ALTIVEC_TRAITS(_TYPE)                                      \
  template <>                                                                  \
  struct __native_type_traits<_TYPE, 16> {                                     \
    using type = __vector _TYPE;                                               \
  }

_SPECIALIZE_ALTIVEC_TRAITS(char);
_SPECIALIZE_ALTIVEC_TRAITS(char16_t);
_SPECIALIZE_ALTIVEC_TRAITS(char32_t);
_SPECIALIZE_ALTIVEC_TRAITS(wchar_t);
_SPECIALIZE_ALTIVEC_TRAITS(signed char);
_SPECIALIZE_ALTIVEC_TRAITS(signed short);
_SPECIALIZE_ALTIVEC_TRAITS(signed int);
_SPECIALIZE_ALTIVEC_TRAITS(signed long);
_SPECIALIZE_ALTIVEC_TRAITS(signed long long);
_SPECIALIZE_ALTIVEC_TRAITS(unsigned char);
_SPECIALIZE_ALTIVEC_TRAITS(unsigned short);
_SPECIALIZE_ALTIVEC_TRAITS(unsigned int);
_SPECIALIZE_ALTIVEC_TRAITS(unsigned long);
_SPECIALIZE_ALTIVEC_TRAITS(unsigned long long);
_SPECIALIZE_ALTIVEC_TRAITS(float);
_SPECIALIZE_ALTIVEC_TRAITS(double);

#undef _SPECIALIZE_ALTIVEC_TRAITS
#elif defined(_DIMSUM_MICROARCH_NEON)
#define _SPECIALIZE_NEON_TRAITS(_TYPE, _NUM_BYTES, _UNDERLYING_TYPE)           \
  template <>                                                                  \
  struct __native_type_traits<_TYPE, _NUM_BYTES> {                             \
    using type = _UNDERLYING_TYPE;                                             \
  }

_SPECIALIZE_NEON_TRAITS(char, 16, int8x16_t);
_SPECIALIZE_NEON_TRAITS(char16_t, 16, int16x8_t);
_SPECIALIZE_NEON_TRAITS(char32_t, 16, int32x4_t);
_SPECIALIZE_NEON_TRAITS(wchar_t, 16, int16x8_t);
_SPECIALIZE_NEON_TRAITS(signed char, 16, int8x16_t);
_SPECIALIZE_NEON_TRAITS(signed short, 16, int16x8_t);
_SPECIALIZE_NEON_TRAITS(signed int, 16, int32x4_t);
_SPECIALIZE_NEON_TRAITS(signed long, 16, int64x2_t);
_SPECIALIZE_NEON_TRAITS(signed long long, 16, int64x2_t);
_SPECIALIZE_NEON_TRAITS(unsigned char, 16, uint8x16_t);
_SPECIALIZE_NEON_TRAITS(unsigned short, 16, uint16x8_t);
_SPECIALIZE_NEON_TRAITS(unsigned int, 16, uint32x4_t);
_SPECIALIZE_NEON_TRAITS(unsigned long, 16, uint64x2_t);
_SPECIALIZE_NEON_TRAITS(unsigned long long, 16, uint64x2_t);
_SPECIALIZE_NEON_TRAITS(float, 16, float32x4_t);
_SPECIALIZE_NEON_TRAITS(double, 16, float64x2_t);
_SPECIALIZE_NEON_TRAITS(char, 8, int8x8_t);
_SPECIALIZE_NEON_TRAITS(char16_t, 8, int16x4_t);
_SPECIALIZE_NEON_TRAITS(char32_t, 8, int32x2_t);
_SPECIALIZE_NEON_TRAITS(wchar_t, 8, int16x4_t);
_SPECIALIZE_NEON_TRAITS(signed char, 8, int8x8_t);
_SPECIALIZE_NEON_TRAITS(signed short, 8, int16x4_t);
_SPECIALIZE_NEON_TRAITS(signed int, 8, int32x2_t);
_SPECIALIZE_NEON_TRAITS(signed long, 8, int64x1_t);
_SPECIALIZE_NEON_TRAITS(signed long long, 8, int64x1_t);
_SPECIALIZE_NEON_TRAITS(unsigned char, 8, uint8x8_t);
_SPECIALIZE_NEON_TRAITS(unsigned short, 8, uint16x4_t);
_SPECIALIZE_NEON_TRAITS(unsigned int, 8, uint32x2_t);
_SPECIALIZE_NEON_TRAITS(unsigned long, 8, uint64x1_t);
_SPECIALIZE_NEON_TRAITS(unsigned long long, 8, uint64x1_t);
_SPECIALIZE_NEON_TRAITS(float, 8, float32x2_t);
_SPECIALIZE_NEON_TRAITS(double, 8, float64x1_t);

#undef _SPECIALIZE_VEC_EXT
#endif // _DIMSUM_MICROARCH_NEON

template <class _Vp, class _Tp, class _Abi>
class __simd_reference {
  static_assert(std::is_same<_Vp, _Tp>::value ||
                    (std::is_same<_Vp, bool>::value &&
                     std::is_unsigned<_Tp>::value),
                "");

  template <class, class>
  friend struct simd;

  template <class, class>
  friend struct simd_mask;

  template <class, class>
  friend class simd_mask;

  __simd_storage<_Tp, _Abi>* __ptr_;
  size_t __index_;

  __simd_reference(__simd_storage<_Tp, _Abi>* __ptr, size_t __index)
      : __ptr_(__ptr), __index_(__index) {}

  __simd_reference(const __simd_reference&) = default;

  static _Vp __to_value_type(_Tp __val) { return __val; }

  static _Tp __from_value_type(_Vp __val) {
    if (std::is_same<_Vp, bool>::value) {
      return __val ? -1 : 0;
    }
    return __val;
  }

public:
  __simd_reference() = delete;
  __simd_reference& operator=(const __simd_reference&) = delete;

  operator _Vp() const { return __to_value_type(__ptr_->__get(__index_)); }

  __simd_reference operator=(_Vp __val) && {
    __ptr_->__set(__index_, __from_value_type(__val));
    return *this;
  }

  template <class _Up>
  __simd_reference operator=(__simd_reference<_Vp, _Up, _Abi>&& __ref) && {
    return std::move(*this) = _Vp(__ref);
  }

  __simd_reference operator++() && {
    return std::move(*this) = __ptr_->__get(__index_) + 1;
  }

  _Vp operator++(int) && {
    auto __val = __ptr_->__get(__index_);
    __ptr_->__set(__index_, __val + 1);
    return __to_value_type(__val);
  }

  __simd_reference operator--() && {
    return std::move(*this) = __ptr_->__get(__index_) - 1;
  }

  _Vp operator--(int) && {
    auto __val = __ptr_->__get(__index_);
    __ptr_->__set(__index_, __val - 1);
    return __to_value_type(__val);
  }

  __simd_reference operator+=(_Vp __val) && {
    return std::move(*this) =
               __from_value_type(__ptr_->__get(__index_) + __val);
  }

  __simd_reference operator-=(_Vp __val) && {
    return std::move(*this) =
               __from_value_type(__ptr_->__get(__index_) - __val);
  }

  __simd_reference operator*=(_Vp __val) && {
    return std::move(*this) =
               __from_value_type(__ptr_->__get(__index_) * __val);
  }

  __simd_reference operator/=(_Vp __val) && {
    return std::move(*this) =
               __from_value_type(__ptr_->__get(__index_) / __val);
  }

  __simd_reference operator%=(_Vp __val) && {
    return std::move(*this) =
               __from_value_type(__ptr_->__get(__index_) % __val);
  }

  __simd_reference operator>>=(_Vp __val) && {
    return std::move(*this) =
               __from_value_type(__ptr_->__get(__index_) >> __val);
  }

  __simd_reference operator<<=(_Vp __val) && {
    return std::move(*this) =
               __from_value_type(__ptr_->__get(__index_) << __val);
  }

  __simd_reference operator&=(_Vp __val) && {
    return std::move(*this) =
               __from_value_type(__ptr_->__get(__index_) & __val);
  }

  __simd_reference operator|=(_Vp __val) && {
    return std::move(*this) =
               __from_value_type(__ptr_->__get(__index_) | __val);
  }

  __simd_reference operator^=(_Vp __val) && {
    return std::move(*this) =
               __from_value_type(__ptr_->__get(__index_) ^ __val);
  }

  friend void swap(__simd_reference&& a, __simd_reference&& b) noexcept {
    _Vp __val = std::move(b);
    std::move(b) = std::move(a);
    std::move(a) = __val;
  }

  friend void swap(_Vp& a, __simd_reference&& b) noexcept {
    swap(std::move(b), a);
  }

  friend void swap(__simd_reference&& a, _Vp& b) noexcept {
    _Vp __val = b;
    b = std::move(a);
    std::move(a) = __val;
  }
};

template <class _To, class _From>
constexpr decltype(_To{std::declval<_From>()}, true)
__is_non_narrowing_convertible_impl(_From) {
  return true;
}

template <class _To>
constexpr bool __is_non_narrowing_convertible_impl(...) {
  return false;
}

template <class _From, class _To>
constexpr typename std::enable_if<std::is_arithmetic<_To>::value &&
                                      std::is_arithmetic<_From>::value,
                                  bool>::type
__is_non_narrowing_arithmetic_convertible() {
  return __is_non_narrowing_convertible_impl<_To>(_From{});
}

template <class _From, class _To>
constexpr typename std::enable_if<!(std::is_arithmetic<_To>::value &&
                                    std::is_arithmetic<_From>::value),
                                  bool>::type
__is_non_narrowing_arithmetic_convertible() {
  return false;
}

template <class _Tp>
constexpr _Tp __variadic_sum() {
  return _Tp{};
}

template <class _Tp, class _Up, class... _Args>
constexpr _Tp __variadic_sum(_Up __first, _Args... __rest) {
  return static_cast<_Tp>(__first) + __variadic_sum<_Tp>(__rest...);
}

template <class _Tp>
struct __nodeduce {
  using type = _Tp;
};

template <class _Tp>
constexpr bool __vectorizable() {
  return std::is_arithmetic<_Tp>::value && !std::is_const<_Tp>::value &&
         !std::is_volatile<_Tp>::value && !std::is_same<_Tp, bool>::value;
}

template <size_t __num_bytes>
struct __unsigned_traits {};

template <>
struct __unsigned_traits<1> {
  using type = uint8_t;
};

template <>
struct __unsigned_traits<2> {
  using type = uint16_t;
};

template <>
struct __unsigned_traits<4> {
  using type = uint32_t;
};

template <>
struct __unsigned_traits<8> {
  using type = uint64_t;
};

#if !defined(_DIMSUM_HAS_NO_INT128)
template <>
struct __unsigned_traits<16> {
  using type = __uint128_t;
};
#endif // !defined(_DIMSUM_HAS_NO_INT128)

#if _DIMSUM_STD_VER > 11
using __simd_plus_op = std::plus<>;
#else
struct __simd_plus_op {
  template <class _T1, class _T2>
  inline auto operator()(_T1&& __t, _T2&& __u) const
      noexcept(noexcept(std::forward<_T1>(__t) + std::forward<_T2>(__u)))
          -> decltype(std::forward<_T1>(__t) + std::forward<_T2>(__u)) {
    return std::forward<_T1>(__t) + std::forward<_T2>(__u);
  }
};
#endif

_DIMSUM_END_NAMESPACE_EXPERIMENTAL_SIMD
_DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL_SIMD_ABI

using scalar = __simd_abi<_StorageKind::_Scalar, 1>;

template <int _Np>
using fixed_size = __simd_abi<_StorageKind::_Array, _Np>;

#if _DIMSUM_STD_VER >= 14 && !defined(_DIMSUM_HAS_NO_VARIABLE_TEMPLATES)
template <class _Tp>
_DIMSUM_INLINE_VAR constexpr size_t max_fixed_size = 64;
#endif

template <class _Tp, size_t _Np>
using __compatible = fixed_size<_Np>;

template <class _Tp, size_t _Np>
using __native =
#ifndef _DIMSUM_HAS_NO_VECTOR_EXTENSION
    __simd_abi<_StorageKind::_VecExt, _Np>;
#else
    __simd_abi<_StorageKind::_Array, _Np>;
#endif // _DIMSUM_HAS_NO_VECTOR_EXTENSION

template <class _Tp>
using compatible = __compatible<_Tp, 16 / sizeof(_Tp)>;

template <class _Tp>
using native = __native<_Tp,
#if defined(_DIMSUM_MICROARCH_AVX)
                        32
#else
                        16
#endif
                            / sizeof(_Tp)>;

// NOTE: _Abis... is the extension proposed by P0820, allowing the APIs to
// propagate _StorageKind during transforming input type(s) to the output type.
template <class _Tp, size_t _Np, class... _Abis>
struct deduce {
  using type = simd_abi::fixed_size<_Np>;
};

template <class _Tp, size_t _Np, _StorageKind __kind, int... __old_size>
struct deduce<_Tp, _Np, __simd_abi<__kind, __old_size>...> {
  using type = __simd_abi<__kind, _Np>;
};

template <class _Tp, size_t _Np, class... _Abis>
using deduce_t = typename deduce<_Tp, _Np, _Abis...>::type;

_DIMSUM_END_NAMESPACE_EXPERIMENTAL_SIMD_ABI
_DIMSUM_BEGIN_NAMESPACE_EXPERIMENTAL_SIMD

template <class _Tp, class _Abi = simd_abi::compatible<_Tp>>
class simd;
template <class _Tp, class _Abi = simd_abi::compatible<_Tp>>
class simd_mask;

struct element_aligned_tag {};
struct vector_aligned_tag {};
template <size_t>
struct overaligned_tag {};
_DIMSUM_INLINE_VAR constexpr element_aligned_tag element_aligned{};
_DIMSUM_INLINE_VAR constexpr vector_aligned_tag vector_aligned{};
#if !defined(_DIMSUM_HAS_NO_VARIABLE_TEMPLATES)
template <size_t _Np>
_DIMSUM_INLINE_VAR constexpr overaligned_tag<_Np> overaligned{};
#endif

// traits [simd.traits]
template <class _Tp>
struct is_abi_tag : std::integral_constant<bool, false> {};

template <_StorageKind __kind, int _Np>
struct is_abi_tag<__simd_abi<__kind, _Np>>
    : std::integral_constant<bool, true> {};

template <class _Tp>
struct is_simd : std::integral_constant<bool, false> {};

template <class _Tp, class _Abi>
struct is_simd<simd<_Tp, _Abi>> : std::integral_constant<bool, true> {};

template <class _Tp>
struct is_simd_mask : std::integral_constant<bool, false> {};

template <class _Tp, class _Abi>
struct is_simd_mask<simd_mask<_Tp, _Abi>> : std::integral_constant<bool, true> {
};

template <class _Tp>
struct is_simd_flag_type : std::integral_constant<bool, false> {};

template <>
struct is_simd_flag_type<element_aligned_tag>
    : std::integral_constant<bool, true> {};

template <>
struct is_simd_flag_type<vector_aligned_tag>
    : std::integral_constant<bool, true> {};

template <size_t _Align>
struct is_simd_flag_type<overaligned_tag<_Align>>
    : std::integral_constant<bool, true> {};

#if _DIMSUM_STD_VER >= 14 && !defined(_DIMSUM_HAS_NO_VARIABLE_TEMPLATES)
template <class _Tp>
_DIMSUM_INLINE_VAR constexpr bool is_abi_tag_v = is_abi_tag<_Tp>::value;
template <class _Tp>
_DIMSUM_INLINE_VAR constexpr bool is_simd_v = is_simd<_Tp>::value;
template <class _Tp>
_DIMSUM_INLINE_VAR constexpr bool is_simd_mask_v = is_simd_mask<_Tp>::value;
template <class _Tp>
_DIMSUM_INLINE_VAR constexpr bool is_simd_flag_type_v =
    is_simd_flag_type<_Tp>::value;
#endif

template <class _Tp, class _Abi = simd_abi::compatible<_Tp>>
struct simd_size;

template <class _Tp, _StorageKind __kind, int _Np>
struct simd_size<_Tp, __simd_abi<__kind, _Np>>
    : std::integral_constant<size_t, _Np> {
  static_assert(
      std::is_arithmetic<_Tp>::value &&
          !std::is_same<typename std::remove_const<_Tp>::type, bool>::value,
      "Element type should be vectorizable");
};

template <class _ValueType, class _Up, class _Flags>
struct __memory_alignment_impl : std::integral_constant<size_t, alignof(_Up)> {
};

template <class _Tp, class _Abi, class _Up>
struct __memory_alignment_impl<simd<_Tp, _Abi>, _Up, vector_aligned_tag>
    : std::integral_constant<size_t, alignof(simd<_Tp, _Abi>)> {};

// TODO: Figure out a useful alignment based on simd_mask load and store
// implementation. Currently, make sure that the buffer is suitable for aligned
// SIMD load.
template <class _Tp, class _Abi, class _Up>
struct __memory_alignment_impl<simd_mask<_Tp, _Abi>, _Up, vector_aligned_tag>
    : std::integral_constant<size_t, alignof(simd<uint8_t, _Abi>)> {};

template <class _ValueType, class _Up, size_t __alignment>
struct __memory_alignment_impl<_ValueType, _Up, overaligned_tag<__alignment>>
    : std::integral_constant<size_t, __alignment> {};

template <class _SimdType, class _Up = typename _SimdType::value_type>
struct memory_alignment
    : __memory_alignment_impl<_SimdType, _Up, vector_aligned_tag> {};

#if _DIMSUM_STD_VER >= 14 && !defined(_DIMSUM_HAS_NO_VARIABLE_TEMPLATES)
template <class _Tp, class _Abi = simd_abi::compatible<_Tp>>
_DIMSUM_INLINE_VAR constexpr size_t simd_size_v = simd_size<_Tp, _Abi>::value;

template <class _SimdType, class _Up = typename _SimdType::value_type>
_DIMSUM_INLINE_VAR constexpr size_t memory_alignment_v =
    memory_alignment<_SimdType, _Up>::value;
#endif

// class template simd [simd.class]
template <class _Tp>
using native_simd = simd<_Tp, simd_abi::native<_Tp>>;
template <class _Tp, int _Np>
using fixed_size_simd = simd<_Tp, simd_abi::fixed_size<_Np>>;

// class template simd_mask [simd.mask.class]
template <class _Tp>
using native_simd_mask = simd_mask<_Tp, simd_abi::native<_Tp>>;

template <class _Tp, int _Np>
using fixed_size_simd_mask = simd_mask<_Tp, simd_abi::fixed_size<_Np>>;

// casts [simd.casts]
template <class _Tp>
struct __static_simd_cast_traits {
  template <class _Up, class _Abi>
  static simd<_Tp, _Abi> __apply(const simd<_Up, _Abi>& __v) {
    return __static_simd_cast_traits<simd<_Tp, _Abi>>::__apply(__v);
  }
};

template <class _Tp, class _NewAbi>
struct __static_simd_cast_traits<simd<_Tp, _NewAbi>> {
  template <class _Up, class _Abi>
  static typename std::enable_if<simd<_Up, _Abi>::size() ==
                                     simd<_Tp, _NewAbi>::size(),
                                 simd<_Tp, _NewAbi>>::type
  __apply(const simd<_Up, _Abi>& __v) {
    simd<_Tp, _NewAbi> __ret;
    for (size_t __i = 0; __i < __v.size(); __i++) {
      __ret[__i] = static_cast<_Tp>(__v[__i]);
    }
    return __ret;
  }
};

template <class _Tp>
struct __simd_cast_traits {
  template <class _Up, class _Abi>
  static typename std::enable_if<
      __is_non_narrowing_arithmetic_convertible<_Up, _Tp>(),
      simd<_Tp, _Abi>>::type
  __apply(const simd<_Up, _Abi>& __v) {
    return __static_simd_cast_traits<_Tp>::__apply(__v);
  }
};

template <class _Tp, class _NewAbi>
struct __simd_cast_traits<simd<_Tp, _NewAbi>> {
  template <class _Up, class _Abi>
  static typename std::enable_if<
      __is_non_narrowing_arithmetic_convertible<_Up, _Tp>() &&
          simd<_Up, _Abi>::size() == simd<_Tp, _NewAbi>::size(),
      simd<_Tp, _NewAbi>>::type
  __apply(const simd<_Up, _Abi>& __v) {
    return __static_simd_cast_traits<simd<_Tp, _NewAbi>>::__apply(__v);
  }
};

template <class _Tp, class _Up, class _Abi>
auto simd_cast(const simd<_Up, _Abi>& __v)
    -> decltype(__simd_cast_traits<_Tp>::__apply(__v)) {
  return __simd_cast_traits<_Tp>::__apply(__v);
}

template <class _Tp, class _Up, class _Abi>
auto static_simd_cast(const simd<_Up, _Abi>& __v)
    -> decltype(__static_simd_cast_traits<_Tp>::__apply(__v)) {
  return __static_simd_cast_traits<_Tp>::__apply(__v);
}

template <class _Tp, class _Abi>
fixed_size_simd<_Tp, simd_size<_Tp, _Abi>::value>
to_fixed_size(const simd<_Tp, _Abi>& __v) noexcept {
  return simd_cast<fixed_size_simd<_Tp, simd_size<_Tp, _Abi>::value>>(__v);
}

// NOTE: As an extension, allow transforming to a native type with a size
// that's not native_simd<T>::size().
template <class _Tp, int _Np>
simd<_Tp, simd_abi::__native<_Tp, _Np>>
to_native(const fixed_size_simd<_Tp, _Np>& __v) noexcept {
  return simd_cast<simd<_Tp, simd_abi::__native<_Tp, _Np>>>(__v);
}

// NOTE: As an extension, allow transforming to a compatible type with a size
// that's not simd<T>::size().
template <class _Tp, int _Np>
simd<_Tp, simd_abi::__compatible<_Tp, _Np>>
to_compatible(const fixed_size_simd<_Tp, _Np>& __v) noexcept {
  return simd_cast<simd<_Tp, simd_abi::__compatible<_Tp, _Np>>>(__v);
}

template <class _TupleType, class _Tp, size_t... __indices>
_TupleType __split_tuple_impl(_Tp** __buffers,
                              dimsum::index_sequence<__indices...>) {
  return _TupleType(typename std::tuple_element<__indices, _TupleType>::type(
      __buffers[__indices], element_aligned_tag())...);
}

#if !defined(_DIMSUM_HAS_NO_VECTOR_EXTENSION) && defined(_DIMSUM_COMPILER_CLANG)
template <size_t... __indices, class _Tp, int __num_element>
simd<_Tp, __simd_abi<_StorageKind::_VecExt, sizeof...(__indices)>>
__simd_shuffle(
    const simd<_Tp, __simd_abi<_StorageKind::_VecExt, __num_element>>& __v,
    const simd<_Tp, __simd_abi<_StorageKind::_VecExt, __num_element>>& __u,
    dimsum::index_sequence<__indices...> = {}) {
  simd<_Tp, __simd_abi<_StorageKind::_VecExt, sizeof...(__indices)>> __ret;
  __ret.__s_.__assign(__builtin_shufflevector(__v.__s_.__raw(),
                                              __u.__s_.__raw(), __indices...));
  return __ret;
}
#endif

template <size_t... __indices, class _Tp, class _Abi1, class _Abi2>
simd<_Tp, simd_abi::deduce_t<_Tp, sizeof...(__indices), _Abi1, _Abi2>>
__simd_shuffle(const simd<_Tp, _Abi1>& __v, const simd<_Tp, _Abi2>& __u,
               dimsum::index_sequence<__indices...> = {}) {
  simd<_Tp, simd_abi::deduce_t<_Tp, sizeof...(__indices), _Abi1, _Abi2>> __ret;
  size_t __i = 0;
  for (size_t __index : {__indices...}) {
    __ret[__i++] =
        __index < __v.size() ? __v[__index] : __u[__index - __v.size()];
  }
  return __ret;
}

template <size_t __first_size, class _Tp, class _Abi, size_t... __indices>
tuple<simd<_Tp, simd_abi::deduce_t<_Tp, __first_size, _Abi>>,
      simd<_Tp, simd_abi::deduce_t<_Tp, simd<_Tp, _Abi>::size() - __first_size,
                                   _Abi>>>
__split_to_two(const simd<_Tp, _Abi>& __v, dimsum::index_sequence<__indices...>) {
  static_assert(__first_size + sizeof...(__indices) == simd<_Tp, _Abi>::size(),
                "");
  return std::make_tuple(
      __simd_shuffle(__v, __v, dimsum::make_index_sequence<__first_size>()),
      __simd_shuffle<(__first_size + __indices)...>(__v, __v));
}

template <size_t __size, class _Tp, class _Abi>
std::tuple<simd<_Tp, _Abi>>
__split_impl(const simd<_Tp, _Abi>& __v,
             std::integral_constant<size_t, __size>) {
  return std::make_tuple(__v);
}

template <size_t __first, size_t... __rest, class _Tp, class _Abi>
tuple<simd<_Tp, simd_abi::deduce_t<_Tp, __first, _Abi>>,
      simd<_Tp, simd_abi::deduce_t<_Tp, __rest, _Abi>>...>
__split_impl(const simd<_Tp, _Abi>& __v,
             std::integral_constant<size_t, __first>,
             std::integral_constant<size_t, __rest>...) {
  auto __res = __split_to_two<__first>(
      __v, dimsum::make_index_sequence<simd<_Tp, _Abi>::size() - __first>());
  return std::tuple_cat(
      std::make_tuple(std::get<0>(__res)),
      __split_impl(std::get<1>(__res),
                   std::integral_constant<size_t, __rest>()...));
}

template <size_t... __sizes, class _Tp, class _Abi>
typename std::enable_if<
    __variadic_sum<size_t>(__sizes...) == simd<_Tp, _Abi>::size(),
    tuple<simd<_Tp, simd_abi::deduce_t<_Tp, __sizes, _Abi>>...>>::type
split(const simd<_Tp, _Abi>& __v) {
  return __split_impl(__v, std::integral_constant<size_t, __sizes>()...);
}

template <class _Tp, class _Abi, class _Abi2, size_t __array_size,
          size_t... __indices>
void __split_by_impl(const simd<_Tp, _Abi>& __v,
                     std::array<simd<_Tp, _Abi2>, __array_size>* __arr,
                     dimsum::index_sequence<__indices...>) {
  auto __tp = split<(__indices, simd<_Tp, _Abi2>::size())...>(__v);
  int __not_used[]{((*__arr)[__indices] = std::get<__indices>(__tp), 0)...};
  (void)__not_used;
}

// NOTE: P0820 extension
template <size_t __array_size, class _Tp, class _Abi>
typename std::enable_if<
    simd_size<_Tp, _Abi>::value % __array_size == 0,
    array<simd<_Tp, simd_abi::deduce_t<
                        _Tp, simd_size<_Tp, _Abi>::value / __array_size, _Abi>>,
          __array_size>>::type
split_by(const simd<_Tp, _Abi>& __v) {
  array<simd<_Tp, simd_abi::deduce_t<
                      _Tp, simd_size<_Tp, _Abi>::value / __array_size, _Abi>>,
        __array_size>
      __ret;
  __split_by_impl(__v, &__ret, dimsum::make_index_sequence<__array_size>());
  return __ret;
}

template <class _SimdType, class _Abi>
typename std::enable_if<
    is_simd<_SimdType>::value &&
        simd_size<typename _SimdType::value_type, _Abi>::value %
                _SimdType::size() ==
            0,
    array<_SimdType, simd_size<typename _SimdType::value_type, _Abi>::value /
                         _SimdType::size()>>::type
split(const simd<typename _SimdType::value_type, _Abi>& __v) {
  return split_by<simd_size<typename _SimdType::value_type, _Abi>::value /
                  _SimdType::size()>(__v);
}

template <class _Tp, class _Abi>
simd<_Tp, _Abi> __concat_variadic(const simd<_Tp, _Abi>& __v) {
  return __v;
}

template <class _Tp, class _Abi, class... _Abis>
simd<_Tp,
     simd_abi::deduce_t<_Tp,
                        __variadic_sum<size_t>(simd_size<_Tp, _Abi>::value,
                                               simd_size<_Tp, _Abis>::value...),
                        _Abi, _Abis...>>
__concat_variadic(const simd<_Tp, _Abi>& __first,
                  const simd<_Tp, _Abis>&... __rest) {
  return __simd_shuffle(
      __first, __concat_variadic(__rest...),
      dimsum::make_index_sequence<__variadic_sum<size_t>(
          simd_size<_Tp, _Abi>::value, simd_size<_Tp, _Abis>::value...)>());
}

template <class _Tp, class... _Abis>
simd<_Tp, simd_abi::deduce_t<
              _Tp, __variadic_sum<size_t>(simd_size<_Tp, _Abis>::value...),
              _Abis...>>
concat(const simd<_Tp, _Abis>&... __vs) {
  return __concat_variadic(__vs...);
}

template <class _Tp, class _Abi, size_t _Np, size_t... __indices>
simd<_Tp, simd_abi::deduce_t<_Tp, _Np * simd<_Tp, _Abi>::size(), _Abi>>
__concat_array(const std::array<simd<_Tp, _Abi>, _Np>& __arr,
               dimsum::index_sequence<__indices...>) {
  return concat(__arr[__indices]...);
}

template <class _Tp, class _Abi, size_t _Np>
simd<_Tp, simd_abi::deduce_t<_Tp, _Np * simd<_Tp, _Abi>::size(), _Abi>>
concat(const std::array<simd<_Tp, _Abi>, _Np>& __arr) {
  return __concat_array(__arr, dimsum::make_index_sequence<_Np>());
}

template <class _Up, class _Tp, class _Abi>
simd<_Up, _Abi> __bit_cast(const simd<_Tp, _Abi>& __v) {
  static_assert(std::is_arithmetic<_Up>::value, "");
  static_assert(sizeof(_Up) == sizeof(_Tp), "");
  simd<_Up, _Abi> __ret;
  for (size_t __i = 0; __i < __v.size(); __i++) {
    _Up __tmp;
    _Tp __elem = __v[__i];
    memcpy(&__tmp, &__elem, sizeof(__elem));
    __ret[__i] = __tmp;
  }
  return __ret;
}

struct __simd_mask_friend {
  template <class _Tp, class _Abi>
  static fixed_size_simd_mask<_Tp, simd_size<_Tp, _Abi>::value>
  __to_fixed_size(const simd_mask<_Tp, _Abi>& __m) noexcept {
    return to_fixed_size(__m.__s_);
  }

  template <class _Tp, int _Np>
  static simd_mask<_Tp, simd_abi::__native<_Tp, _Np>>
  __to_native(const fixed_size_simd_mask<_Tp, _Np>& __m) noexcept {
    return to_native(__m.__s_);
  }

  template <class _Tp, int _Np>
  static simd_mask<_Tp, simd_abi::__compatible<_Tp, _Np>>
  __to_compatible(const fixed_size_simd_mask<_Tp, _Np>& __m) noexcept {
    return to_compatible(__m.__s_);
  }

  template <class _ResType, class _TupleType, size_t... __indices>
  static _ResType __split_impl(const _TupleType& __t,
                               dimsum::index_sequence<__indices...>) {
    return _ResType(std::get<__indices>(__t)...);
  }

  template <size_t... __sizes, class _Tp, class _Abi>
  static typename std::enable_if<
      __variadic_sum<size_t>(__sizes...) == simd<_Tp, _Abi>::size(),
      tuple<simd_mask<_Tp, simd_abi::deduce_t<_Tp, __sizes, _Abi>>...>>::type
  __split(const simd_mask<_Tp, _Abi>& __m) {
    return __split_impl<
        tuple<simd_mask<_Tp, simd_abi::deduce_t<_Tp, __sizes, _Abi>>...>>(
        split<__sizes...>(__m.__s_),
        dimsum::make_index_sequence<sizeof...(__sizes)>());
  }

  template <size_t __array_size, class _Tp, class _Abi>
  static typename std::enable_if<
      simd_size<_Tp, _Abi>::value % __array_size == 0,
      array<simd_mask<_Tp, simd_abi::deduce_t<
                               _Tp, simd_size<_Tp, _Abi>::value / __array_size,
                               _Abi>>,
            __array_size>>::type
  __split_by(const simd_mask<_Tp, _Abi>& __m) {
    auto __arr = split_by<__array_size>(__m.__s_);
    array<simd_mask<_Tp,
                    simd_abi::deduce_t<
                        _Tp, simd_size<_Tp, _Abi>::value / __array_size, _Abi>>,
          __array_size>
        __ret;
    for (size_t __i = 0; __i < __ret.size(); __i++) {
      __ret[__i] = __arr[__i];
    }
    return __ret;
  }

  template <class _MaskType, class _Abi>
  static typename std::enable_if<
      is_simd_mask<_MaskType>::value &&
          simd_size<typename _MaskType::value_type, _Abi>::value %
                  _MaskType::size() ==
              0,
      array<_MaskType, simd_size<typename _MaskType::value_type, _Abi>::value /
                           _MaskType::size()>>::type
  __split(const simd_mask<typename _MaskType::value_type, _Abi>& __m) {
    return split<typename _MaskType::simd_type>(__m.__s_);
  }

  template <class _Tp, class... _Abis>
  static simd_mask<
      _Tp, simd_abi::deduce_t<
               _Tp, __variadic_sum<size_t>(simd_size<_Tp, _Abis>::value...),
               _Abis...>>
  __concat(const simd_mask<_Tp, _Abis>&... __ms) {
    return concat(__ms.__s_...);
  }

  template <class _Tp, class _Abi, size_t _Np>
  static simd_mask<
      _Tp, simd_abi::deduce_t<_Tp, _Np * simd_size<_Tp, _Abi>::value, _Abi>>
  __concat(const std::array<simd_mask<_Tp, _Abi>, _Np>& __ms) {
    std::array<decltype(__ms[0].__s_), _Np> __arr;
    for (size_t __i = 0; __i < __ms.size(); __i++) {
      __arr[__i] = __ms[__i].__s_;
    }
    return concat(__arr);
  }

  template <class _Tp, class _Abi>
  static simd<_Tp, _Abi> __simd_select(const simd<_Tp, _Abi>& __false_values,
                                       const simd<_Tp, _Abi>& __true_values,
                                       const simd_mask<_Tp, _Abi>& __m) {
    using __unsigned = typename __unsigned_traits<sizeof(_Tp)>::type;
    return __bit_cast<_Tp>(
        (__bit_cast<__unsigned>(__false_values) & ~__m.__s_) +
        (__bit_cast<__unsigned>(__true_values) & __m.__s_));
  }

  template <class _Tp, class _Abi>
  static simd_mask<_Tp, _Abi>
  __simd_select(const simd_mask<_Tp, _Abi>& __false_values,
                const simd_mask<_Tp, _Abi>& __true_values,
                const simd_mask<_Tp, _Abi>& __m) {
    using __unsigned = typename __unsigned_traits<sizeof(_Tp)>::type;
    return __simd_select(__false_values.__s_, __true_values.__s_,
                         simd_mask<__unsigned, _Abi>(__m.__s_));
  }

  template <class _Tp>
  static _Tp __simd_select(_Tp __false_value, _Tp __true_value, bool __m) {
    return __m ? __true_value : __false_value;
  }
};

template <class _Tp, class _Abi>
fixed_size_simd_mask<_Tp, simd_size<_Tp, _Abi>::value>
to_fixed_size(const simd_mask<_Tp, _Abi>& __m) noexcept {
  return __simd_mask_friend::__to_fixed_size(__m);
}

template <class _Tp, int _Np>
simd_mask<_Tp, simd_abi::__native<_Tp, _Np>>
to_native(const fixed_size_simd_mask<_Tp, _Np>& __m) noexcept {
  return __simd_mask_friend::__to_native(__m);
}

template <class _Tp, int _Np>
simd_mask<_Tp, simd_abi::__compatible<_Tp, _Np>>
to_compatible(const fixed_size_simd_mask<_Tp, _Np>& __m) noexcept {
  return __simd_mask_friend::__to_compatible(__m);
}

template <size_t... __sizes, class _Tp, class _Abi>
typename std::enable_if<
    __variadic_sum<size_t>(__sizes...) == simd<_Tp, _Abi>::size(),
    tuple<simd_mask<_Tp, simd_abi::deduce_t<_Tp, __sizes, _Abi>>...>>::type
split(const simd_mask<_Tp, _Abi>& __m) {
  return __simd_mask_friend::__split<__sizes...>(__m);
}

template <size_t __array_size, class _Tp, class _Abi>
static typename std::enable_if<
    simd_size<_Tp, _Abi>::value % __array_size == 0,
    array<simd_mask<_Tp,
                    simd_abi::deduce_t<
                        _Tp, simd_size<_Tp, _Abi>::value / __array_size, _Abi>>,
          __array_size>>::type
split_by(const simd_mask<_Tp, _Abi>& __m) {
  return __simd_mask_friend::__split_by<__array_size>(__m);
}

template <class _MaskType, class _Abi>
static typename std::enable_if<
    is_simd_mask<_MaskType>::value &&
        simd_size<typename _MaskType::value_type, _Abi>::value %
                _MaskType::size() ==
            0,
    array<_MaskType, simd_size<typename _MaskType::value_type, _Abi>::value /
                         _MaskType::size()>>::type
split(const simd_mask<typename _MaskType::value_type, _Abi>& __m) {
  return __simd_mask_friend::__split<_MaskType>(__m);
}

template <class _Tp, class... _Abis>
simd_mask<_Tp, simd_abi::deduce_t<
                   _Tp, __variadic_sum<size_t>(simd_size<_Tp, _Abis>::value...),
                   _Abis...>>
concat(const simd_mask<_Tp, _Abis>&... __ms) {
  return __simd_mask_friend::__concat(__ms...);
}

template <class _Tp, class _Abi, size_t _Np>
simd_mask<_Tp, simd_abi::deduce_t<_Tp, _Np * simd_size<_Tp, _Abi>::value, _Abi>>
concat(const std::array<simd_mask<_Tp, _Abi>, _Np>& __ms) {
  return __simd_mask_friend::__concat(__ms);
}

// reductions [simd.mask.reductions]
template <class _Tp, class _Abi>
bool all_of(const simd_mask<_Tp, _Abi>& __m) noexcept {
  bool __ret = true;
  for (size_t __i = 0; __i < __m.size(); __i++) {
    __ret &= __m[__i];
  }
  return __ret;
}

template <class _Tp, class _Abi>
bool any_of(const simd_mask<_Tp, _Abi>& __m) noexcept {
  bool __ret = false;
  for (size_t __i = 0; __i < __m.size(); __i++) {
    __ret |= __m[__i];
  }
  return __ret;
}

template <class _Tp, class _Abi>
bool none_of(const simd_mask<_Tp, _Abi>& __m) noexcept {
  return !any_of(__m);
}

template <class _Tp, class _Abi>
bool some_of(const simd_mask<_Tp, _Abi>& __m) noexcept {
  return !all_of(__m) && !none_of(__m);
}

template <class _Tp, class _Abi>
int popcount(const simd_mask<_Tp, _Abi>& __m) noexcept {
  int __ret = 0;
  for (size_t __i = 0; __i < __m.size(); __i++) {
    __ret += __m[__i];
  }
  return __ret;
}

template <class _Tp, class _Abi>
int find_first_set(const simd_mask<_Tp, _Abi>& __m) {
  for (size_t __i = 0; __i < __m.size(); __i++) {
    if (__m[__i]) {
      return __i;
    }
  }
  return __m.size();
}

template <class _Tp, class _Abi>
int find_last_set(const simd_mask<_Tp, _Abi>& __m) {
  for (int __i = __m.size() - 1; __i >= 0; __i--) {
    if (__m[__i]) {
      return __i;
    }
  }
  return __m.size();
}

template <class _Tp>
typename std::enable_if<std::is_same<_Tp, bool>::value, bool>::type
all_of(_Tp __v) noexcept {
  return __v;
}

template <class _Tp>
typename std::enable_if<std::is_same<_Tp, bool>::value, bool>::type
any_of(_Tp __v) noexcept {
  return __v;
}

template <class _Tp>
typename std::enable_if<std::is_same<_Tp, bool>::value, bool>::type
none_of(_Tp __v) noexcept {
  return !__v;
}

template <class _Tp>
typename std::enable_if<std::is_same<_Tp, bool>::value, bool>::type
some_of(_Tp) noexcept {
  return false;
}

template <class _Tp>
typename std::enable_if<std::is_same<_Tp, bool>::value, int>::type
popcount(_Tp __v) noexcept {
  return __v;
}

template <class _Tp>
typename std::enable_if<std::is_same<_Tp, bool>::value, int>::type
find_first_set(_Tp) noexcept {
  return 0;
}

template <class _Tp>
typename std::enable_if<std::is_same<_Tp, bool>::value, int>::type
find_last_set(_Tp) noexcept {
  return 0;
}

template <class _Tp, class _Abi, size_t... __indices>
std::array<
    simd<_Tp, simd_abi::deduce_t<_Tp, simd<_Tp, _Abi>::size() / 2, _Abi>>, 2>
__deinterleave_impl(const simd<_Tp, _Abi>& __v,
                    dimsum::index_sequence<__indices...>) {
  return {{__simd_shuffle<(2 * __indices)...>(__v, __v),
           __simd_shuffle<(2 * __indices + 1)...>(__v, __v)}};
}

template <class _Tp, class _Abi>
std::array<
    simd<_Tp, simd_abi::deduce_t<_Tp, simd<_Tp, _Abi>::size() / 2, _Abi>>, 2>
__deinterleave(const simd<_Tp, _Abi>& __v) {
  static_assert(simd<_Tp, _Abi>::size() % 2 == 0, "");
  return __deinterleave_impl(
      __v, dimsum::make_index_sequence<simd<_Tp, _Abi>::size() / 2>());
}

// reductions [simd.reductions]
template <class _SimdType, class _BinaryOp>
typename std::enable_if<_SimdType::size() == 1,
                        typename _SimdType::value_type>::type
__reduce(const _SimdType& __v, _BinaryOp) {
  return __v[0];
}

template <class _SimdType, class _BinaryOp>
typename std::enable_if<(_SimdType::size() > 1 && is_simd<_SimdType>::value &&
                         __floor_pow_of_2(_SimdType::size()) ==
                             _SimdType::size()),
                        typename _SimdType::value_type>::type
__reduce(const _SimdType& __v, _BinaryOp __op) {
  if (std::is_same<_BinaryOp, __simd_plus_op>::value) {
    using _Tp = typename _SimdType::value_type;
    if (std::is_integral<_Tp>::value && sizeof(_Tp) < 8) {
      auto __arr = __deinterleave(__v);
      return __reduce(__arr[0] + __arr[1], __op);
    }
  }
  auto __arr = split_by<2>(__v);
  return __reduce(__op(__arr[0], __arr[1]), __op);
}

template <class _SimdType, class _BinaryOp>
typename std::enable_if<(_SimdType::size() > 1 &&
                         !(is_simd<_SimdType>::value &&
                           __floor_pow_of_2(_SimdType::size()) ==
                               _SimdType::size())),
                        typename _SimdType::value_type>::type
__reduce(const _SimdType& __v, _BinaryOp __op) {
  auto __acc = __v[0];
  for (size_t __i = 1; __i < __v.size(); __i++) {
    __acc = __op(__acc, __v[__i]);
  }
  return __acc;
}

template <class _SimdType>
typename std::enable_if<_SimdType::size() == 1,
                        typename _SimdType::value_type>::type
__hmin(const _SimdType& __v) {
  return __v[0];
}

template <class _SimdType>
typename std::enable_if<(_SimdType::size() > 1 && is_simd<_SimdType>::value &&
                         __floor_pow_of_2(_SimdType::size()) ==
                             _SimdType::size()),
                        typename _SimdType::value_type>::type
__hmin(const _SimdType& __v) {
  auto __arr = split_by<2>(__v);
  return __hmin(min(__arr[0], __arr[1]));
}

template <class _SimdType>
typename std::enable_if<(_SimdType::size() > 1 &&
                         !(is_simd<_SimdType>::value &&
                           __floor_pow_of_2(_SimdType::size()) ==
                               _SimdType::size())),
                        typename _SimdType::value_type>::type
__hmin(const _SimdType& __v) {
  auto __acc = __v[0];
  for (size_t __i = 1; __i < __v.size(); __i++) {
    __acc = __acc > __v[__i] ? __v[__i] : __acc;
  }
  return __acc;
}

template <class _SimdType>
typename std::enable_if<_SimdType::size() == 1,
                        typename _SimdType::value_type>::type
__hmax(const _SimdType& __v) {
  return __v[0];
}

template <class _SimdType>
typename std::enable_if<(_SimdType::size() > 1 && is_simd<_SimdType>::value &&
                         __floor_pow_of_2(_SimdType::size()) ==
                             _SimdType::size()),
                        typename _SimdType::value_type>::type
__hmax(const _SimdType& __v) {
  auto __arr = split_by<2>(__v);
  return __hmax(max(__arr[0], __arr[1]));
}

template <class _SimdType>
typename std::enable_if<(_SimdType::size() > 1 &&
                         !(is_simd<_SimdType>::value &&
                           __floor_pow_of_2(_SimdType::size()) ==
                               _SimdType::size())),
                        typename _SimdType::value_type>::type
__hmax(const _SimdType& __v) {
  auto __acc = __v[0];
  for (size_t __i = 1; __i < __v.size(); __i++) {
    __acc = __acc < __v[__i] ? __v[__i] : __acc;
  }
  return __acc;
}

template <class _Tp, class _Abi, class _BinaryOp = __simd_plus_op>
_Tp reduce(const simd<_Tp, _Abi>& __v, _BinaryOp __op = _BinaryOp()) {
  return __reduce(__v, __op);
}

template <class _Tp, class _Abi>
_Tp hmin(const simd<_Tp, _Abi>& __v) {
  return __hmin(__v);
}

template <class _Tp, class _Abi>
_Tp hmax(const simd<_Tp, _Abi>& __v) {
  return __hmax(__v);
}

// algorithms [simd.alg]
template <class _Tp, class _Abi>
// Add `inline` keyword until LLVM PR/36495 is fixed
inline simd<_Tp, _Abi> min(const simd<_Tp, _Abi>& __a,
                           const simd<_Tp, _Abi>& __b) noexcept {
  simd<_Tp, _Abi> __v;
  _DIMSUM_UNROLL for (size_t __i = 0; __i < __v.size(); __i++) {
    __v[__i] = std::min(__a[__i], __b[__i]);
  }
  return __v;
}

template <class _Tp, class _Abi>
// Add `inline` keyword until LLVM PR/36495 is fixed
inline simd<_Tp, _Abi> max(const simd<_Tp, _Abi>& __a,
                           const simd<_Tp, _Abi>& __b) noexcept {
  simd<_Tp, _Abi> __v;
  _DIMSUM_UNROLL for (size_t __i = 0; __i < __v.size(); __i++) {
    __v[__i] = std::max(__a[__i], __b[__i]);
  }
  return __v;
}

template <class _Tp, class _Abi>
std::pair<simd<_Tp, _Abi>, simd<_Tp, _Abi>>
minmax(const simd<_Tp, _Abi>& __a, const simd<_Tp, _Abi>& __b) noexcept {
  return std::make_pair(min(__a, __b), max(__a, __b));
}

template <class _Tp, class _Abi>
simd<_Tp, _Abi> clamp(const simd<_Tp, _Abi>& __v, const simd<_Tp, _Abi>& __lo,
                      const simd<_Tp, _Abi>& __hi) {
  return min(max(__v, __lo), __hi);
}

// [simd.class]
template <class _Tp, class _Abi>
class simd {
public:
  using value_type = _Tp;
  using reference = __simd_reference<_Tp, _Tp, _Abi>;
  using mask_type = simd_mask<_Tp, _Abi>;
  using abi_type = _Abi;

  simd() = default;
  simd(const simd&) = default;
  simd& operator=(const simd&) = default;

  static constexpr size_t size() noexcept {
    return simd_size<_Tp, _Abi>::value;
  }

  template <class, class>
  friend class simd;

  template <class, class>
  friend class simd_mask;

private:
  __simd_storage<_Tp, _Abi> __s_;

  template <class _Up>
  static constexpr bool __can_broadcast() {
    return (std::is_arithmetic<_Up>::value &&
            __is_non_narrowing_arithmetic_convertible<_Up, _Tp>()) ||
           (!std::is_arithmetic<_Up>::value &&
            std::is_convertible<_Up, _Tp>::value) ||
           std::is_same<typename std::remove_const<_Up>::type, int>::value ||
           (std::is_same<typename std::remove_const<_Up>::type,
                         unsigned int>::value &&
            std::is_unsigned<_Tp>::value);
  }

  template <class _Generator, size_t... __indices>
  static constexpr decltype(
      std::forward_as_tuple(std::declval<_Generator>()(
          std::integral_constant<size_t, __indices>())...),
      bool())
  __can_generate(dimsum::index_sequence<__indices...>) {
    return !__variadic_sum<bool>(
        !__can_broadcast<decltype(std::declval<_Generator>()(
            std::integral_constant<size_t, __indices>()))>()...);
  }

  template <class _Generator>
  static bool __can_generate(...) {
    return false;
  }

  template <class _Generator, size_t... __indices>
  void __generator_init(_Generator&& __g, dimsum::index_sequence<__indices...>) {
    int __not_used[]{
        ((*this)[__indices] = __g(std::integral_constant<size_t, __indices>()),
         0)...};
    (void)__not_used;
  }

  template <size_t __alignment, class _Up>
  void __copy_from_impl(const _Up* __buffer
                        __attribute__((align_value(__alignment)))) {
    if (std::is_same<_Tp, _Up>::value) {
      memcpy(&__s_, __buffer, sizeof(_Tp) * size());
    } else {
      for (size_t __i = 0; __i < size(); __i++) {
        (*this)[__i] = static_cast<_Tp>(__buffer[__i]);
      }
    }
  }

  template <size_t __alignment, class _Up>
  void __copy_to_impl(_Up* __buffer
                      __attribute__((align_value(__alignment)))) const {
    if (std::is_same<_Tp, _Up>::value) {
      memcpy(__buffer, &__s_, sizeof(_Tp) * size());
    } else {
      for (size_t __i = 0; __i < size(); __i++) {
        __buffer[__i] = static_cast<_Up>((*this)[__i]);
      }
    }
  }

  static simd __from_storage(__simd_storage<_Tp, _Abi> __s) {
    simd __v;
    __v.__s_ = __s;
    return __v;
  }

  // The actual implementation for all relational operators due to how `friend`
  // works. See CWG 1699.
  static mask_type __cmp_eq_impl(const simd& __a, const simd& __b) {
    using __element = typename __unsigned_traits<sizeof(_Tp)>::type;
    return mask_type(simd<__element, _Abi>::__from_storage(
        __simd_storage<__element, _Abi>::__cmp_eq(__a.__s_, __b.__s_)));
  }

  static mask_type __cmp_ne_impl(const simd& __a, const simd& __b) {
    using __element = typename __unsigned_traits<sizeof(_Tp)>::type;
    return mask_type(simd<__element, _Abi>::__from_storage(
        __simd_storage<__element, _Abi>::__cmp_ne(__a.__s_, __b.__s_)));
  }

  static mask_type __cmp_ge_impl(const simd& __a, const simd& __b) {
    using __element = typename __unsigned_traits<sizeof(_Tp)>::type;
    return mask_type(simd<__element, _Abi>::__from_storage(
        __simd_storage<__element, _Abi>::__cmp_ge(__a.__s_, __b.__s_)));
  }

  static mask_type __cmp_le_impl(const simd& __a, const simd& __b) {
    using __element = typename __unsigned_traits<sizeof(_Tp)>::type;
    return mask_type(simd<__element, _Abi>::__from_storage(
        __simd_storage<__element, _Abi>::__cmp_le(__a.__s_, __b.__s_)));
  }

  static mask_type __cmp_gt_impl(const simd& __a, const simd& __b) {
    using __element = typename __unsigned_traits<sizeof(_Tp)>::type;
    return mask_type(simd<__element, _Abi>::__from_storage(
        __simd_storage<__element, _Abi>::__cmp_gt(__a.__s_, __b.__s_)));
  }

  static mask_type __cmp_lt_impl(const simd& __a, const simd& __b) {
    using __element = typename __unsigned_traits<sizeof(_Tp)>::type;
    return mask_type(simd<__element, _Abi>::__from_storage(
        __simd_storage<__element, _Abi>::__cmp_lt(__a.__s_, __b.__s_)));
  }

public:
  using __native_type = typename std::conditional<
      std::is_same<void, typename __native_type_traits<
                             _Tp, sizeof(_Tp) * size()>::type>::value,
      typename __simd_storage<_Tp, _Abi>::__raw_type,
      typename __native_type_traits<_Tp, sizeof(_Tp) * size()>::type>::type;

  // implicit type conversion constructor
  template <class _Up,
            class = typename std::enable_if<
                std::is_same<_Abi, simd_abi::fixed_size<size()>>::value &&
                __is_non_narrowing_arithmetic_convertible<_Up, _Tp>()>::type>
  simd(const simd<_Up, simd_abi::fixed_size<size()>>& __v) {
    for (size_t __i = 0; __i < size(); __i++) {
      (*this)[__i] = static_cast<_Tp>(__v[__i]);
    }
  }

  // implicit broadcast constructor
  template <class _Up,
            class = typename std::enable_if<__can_broadcast<_Up>()>::type>
  simd(_Up&& __rv) {
    auto __val = static_cast<_Tp>(__rv);
    for (size_t __i = 0; __i < size(); __i++) {
      (*this)[__i] = __val;
    }
  }

  // generator constructor
  template <class _Generator,
            int = typename std::enable_if<
                __can_generate<_Generator>(dimsum::make_index_sequence<size()>()),
                int>::type()>
  explicit simd(_Generator&& __g) {
    __generator_init(std::forward<_Generator>(__g),
                     dimsum::make_index_sequence<size()>());
  }

  simd(__native_type __r) {
    __s_.__assign(typename __simd_storage<_Tp, _Abi>::__raw_type(__r));
  }

  __native_type __raw() const { return __native_type(__s_.__raw()); }

  operator __native_type() const { return __raw(); }

  // load constructor
  template <
      class _Up, class _Flags,
      class = typename std::enable_if<__vectorizable<_Up>()>::type,
      class = typename std::enable_if<is_simd_flag_type<_Flags>::value>::type>
  simd(const _Up* __buffer, _Flags) {
    __copy_from_impl<__memory_alignment_impl<simd, _Up, _Flags>::value>(
        __buffer);
  }

  // loads [simd.load]
  template <class _Up, class _Flags>
  typename std::enable_if<__vectorizable<_Up>() &&
                          is_simd_flag_type<_Flags>::value>::type
  copy_from(const _Up* __buffer, _Flags) {
    *this = simd(__buffer, _Flags());
  }

  // stores [simd.store]
  template <class _Up, class _Flags>
  typename std::enable_if<__vectorizable<_Up>() &&
                          is_simd_flag_type<_Flags>::value>::type
  copy_to(_Up* __buffer, _Flags) const {
    __copy_to_impl<__memory_alignment_impl<simd, _Up, _Flags>::value>(__buffer);
  }

  // scalar access [simd.subscr]
  reference operator[](size_t __i) { return reference(&__s_, __i); }

  value_type operator[](size_t __i) const { return __s_.__get(__i); }

  // unary operators [simd.unary]
  simd& operator++() {
    *this += simd(1);
    return *this;
  }

  simd operator++(int) {
    auto __tmp = *this;
    ++*this;
    return __tmp;
  }

  simd& operator--() {
    *this -= simd(1);
    return *this;
  }

  simd operator--(int) {
    auto __tmp = *this;
    --*this;
    return __tmp;
  }

  mask_type operator!() const { return *this == simd(0); }

  simd operator~() const {
    return __from_storage(__simd_storage<_Tp, _Abi>::__not(this->__s_));
  }

  simd operator+() const { return *this; }

  simd operator-() const {
    return __from_storage(__simd_storage<_Tp, _Abi>::__neg(this->__s_));
  }

  // binary operators [simd.binary]
  // TODO: currently the operators are not SFINAEed. Fix it.
  friend simd operator+(const simd& __a, const simd& __b) {
    return __from_storage(__simd_storage<_Tp, _Abi>::__add(__a.__s_, __b.__s_));
  }

  friend simd operator-(const simd& __a, const simd& __b) {
    return __from_storage(__simd_storage<_Tp, _Abi>::__sub(__a.__s_, __b.__s_));
  }

  friend simd operator*(const simd& __a, const simd& __b) {
    return __from_storage(__simd_storage<_Tp, _Abi>::__mul(__a.__s_, __b.__s_));
  }

  friend simd operator/(const simd& __a, const simd& __b) {
    return __from_storage(__simd_storage<_Tp, _Abi>::__div(__a.__s_, __b.__s_));
  }

  friend simd operator%(const simd& __a, const simd& __b) {
    return __from_storage(__simd_storage<_Tp, _Abi>::__mod(__a.__s_, __b.__s_));
  }

  friend simd operator&(const simd& __a, const simd& __b) {
    return __from_storage(__simd_storage<_Tp, _Abi>::__and(__a.__s_, __b.__s_));
  }

  friend simd operator|(const simd& __a, const simd& __b) {
    return __from_storage(__simd_storage<_Tp, _Abi>::__or(__a.__s_, __b.__s_));
  }

  friend simd operator^(const simd& __a, const simd& __b) {
    return __from_storage(__simd_storage<_Tp, _Abi>::__xor(__a.__s_, __b.__s_));
  }

  friend simd operator<<(const simd& __a, const simd& __b) {
    return __from_storage(__simd_storage<_Tp, _Abi>::__shl(__a.__s_, __b.__s_));
  }

  friend simd operator>>(const simd& __a, const simd& __b) {
    return __from_storage(__simd_storage<_Tp, _Abi>::__shr(__a.__s_, __b.__s_));
  }

  friend simd operator<<(const simd& __a, int __offset) {
    return __a << simd(__offset);
  }

  friend simd operator>>(const simd& __a, int __offset) {
    return __a >> simd(__offset);
  }

  friend simd& operator+=(simd& __a, const simd& __b) {
    return __a = __a + __b;
  }

  friend simd& operator-=(simd& __a, const simd& __b) {
    return __a = __a - __b;
  }

  friend simd& operator*=(simd& __a, const simd& __b) {
    return __a = __a * __b;
  }

  friend simd& operator/=(simd& __a, const simd& __b) {
    return __a = __a / __b;
  }

  friend simd& operator%=(simd& __a, const simd& __b) {
    return __a = __a % __b;
  }

  friend simd& operator&=(simd& __a, const simd& __b) {
    return __a = __a & __b;
  }

  friend simd& operator|=(simd& __a, const simd& __b) {
    return __a = __a | __b;
  }

  friend simd& operator^=(simd& __a, const simd& __b) {
    return __a = __a ^ __b;
  }

  friend simd& operator<<=(simd& __a, const simd& __b) {
    return __a = __a << __b;
  }

  friend simd& operator>>=(simd& __a, const simd& __b) {
    return __a = __a >> __b;
  }

  friend simd& operator<<=(simd& __a, int __offset) {
    return __a = __a << __offset;
  }

  friend simd& operator>>=(simd& __a, int __offset) {
    return __a = __a >> __offset;
  }

  // compares [simd.comparison]
  friend mask_type operator==(const simd& __a, const simd& __b) {
    return __cmp_eq_impl(__a, __b);
  }

  friend mask_type operator!=(const simd& __a, const simd& __b) {
    return __cmp_ne_impl(__a, __b);
  }

  friend mask_type operator>=(const simd& __a, const simd& __b) {
    return __cmp_ge_impl(__a, __b);
  }

  friend mask_type operator<=(const simd& __a, const simd& __b) {
    return __cmp_le_impl(__a, __b);
  }

  friend mask_type operator>(const simd& __a, const simd& __b) {
    return __cmp_gt_impl(__a, __b);
  }

  friend mask_type operator<(const simd& __a, const simd& __b) {
    return __cmp_lt_impl(__a, __b);
  }

#if !defined(_DIMSUM_HAS_NO_VECTOR_EXTENSION) && defined(_DIMSUM_COMPILER_CLANG)
  template <size_t... __indices, class _Up, int __num_element>
  friend simd<_Up, __simd_abi<_StorageKind::_VecExt, sizeof...(__indices)>>
  __simd_shuffle(
      const simd<_Up, __simd_abi<_StorageKind::_VecExt, __num_element>>& __v,
      const simd<_Up, __simd_abi<_StorageKind::_VecExt, __num_element>>& __u,
      dimsum::index_sequence<__indices...>);
#endif
};

// [simd.mask.class]
template <class _Tp, class _Abi>
class simd_mask {
  using __element_type = typename __unsigned_traits<sizeof(_Tp)>::type;

  simd<__element_type, _Abi> __s_;

  simd_mask(const simd<__element_type, _Abi>& __mask) : __s_(__mask) {}

  friend struct __simd_mask_friend;

  template <class, class>
  friend class simd;

  // Use a non-member function, only because Clang 3.8 crashes with a member function.
  template <size_t __alignment>
  static void __copy_from_impl(simd_mask* __mask, const bool* __buffer
                               __attribute__((align_value(__alignment)))) {
    for (size_t __i = 0; __i < size(); __i++) {
      (*__mask)[__i] = __buffer[__i];
    }
  }

  // Use a non-member function, only because Clang 3.8 crashes with a member function.
  template <size_t __alignment>
  static void __copy_to_impl(const simd_mask* __mask, bool* __buffer
                             __attribute__((align_value(__alignment)))) {
    for (size_t __i = 0; __i < size(); __i++) {
      __buffer[__i] = (*__mask)[__i];
    }
  }

public:
  using value_type = bool;
  using reference = __simd_reference<bool, __element_type, _Abi>;
  using simd_type = simd<_Tp, _Abi>;
  using abi_type = _Abi;
  using __native_type = typename simd<__element_type, _Abi>::__native_type;

  static constexpr size_t size() noexcept {
    return simd<__element_type, _Abi>::size();
  }

  simd_mask() = default;

  // broadcast constructor
  explicit simd_mask(value_type __val) noexcept {
    for (size_t __i = 0; __i < size(); __i++) {
      (*this)[__i] = __val;
    }
  }

  // implicit type conversion constructor
  template <
      class _Up,
      class = typename std::enable_if<
          !std::is_void<_Up>::value &&
          std::is_same<abi_type, simd_abi::fixed_size<size()>>::value>::type>
  simd_mask(const simd_mask<_Up, simd_abi::fixed_size<size()>>& __v) noexcept {
    for (size_t __i = 0; __i < size(); __i++) {
      (*this)[__i] = __v[__i];
    }
  }

  // load constructor
  template <class _Flags, class = typename std::enable_if<
                              is_simd_flag_type<_Flags>::value>::type>
  simd_mask(const value_type* __buffer, _Flags) {
    __copy_from_impl<__memory_alignment_impl<simd_mask, bool, _Flags>::value>(
        this, __buffer);
  }

  simd_mask(__native_type __r) : __s_(__r) {}

  __native_type __raw() const { return __s_.__raw(); }

  operator __native_type() const { return __raw(); }

  // loads [simd.mask.copy]
  template <class _Flags>
  typename std::enable_if<is_simd_flag_type<_Flags>::value>::type
  copy_from(const value_type* __buffer, _Flags) {
    *this = simd_mask(__buffer, _Flags());
  }

  template <class _Flags>
  typename std::enable_if<is_simd_flag_type<_Flags>::value>::type
  copy_to(value_type* __buffer, _Flags) const {
    __copy_to_impl<__memory_alignment_impl<simd_mask, bool, _Flags>::value>(
        this, __buffer);
  }

  // scalar access [simd.mask.subscr]
  reference operator[](size_t __i) { return reference(&__s_.__s_, __i); }

  value_type operator[](size_t __i) const { return __s_[__i]; }

  // unary operators [simd.mask.unary]
  simd_mask operator!() const noexcept { return simd_mask(~__s_); }

  // simd_mask binary operators [simd.mask.binary]
  friend simd_mask operator&&(const simd_mask& __a,
                              const simd_mask& __b) noexcept {
    return __a & __b;
  }

  friend simd_mask operator||(const simd_mask& __a,
                              const simd_mask& __b) noexcept {
    return __a | __b;
  }

  friend simd_mask operator&(const simd_mask& __a,
                             const simd_mask& __b) noexcept {
    return simd_mask(__a.__s_ & __b.__s_);
  }

  friend simd_mask operator|(const simd_mask& __a,
                             const simd_mask& __b) noexcept {
    return simd_mask(__a.__s_ | __b.__s_);
  }

  friend simd_mask operator^(const simd_mask& __a,
                             const simd_mask& __b) noexcept {
    return simd_mask(__a.__s_ ^ __b.__s_);
  }

  // simd_mask compound assignment [simd.mask.cassign]
  friend simd_mask& operator&=(simd_mask& __a, const simd_mask& __b) noexcept {
    return __a = __a & __b;
  }

  friend simd_mask& operator|=(simd_mask& __a, const simd_mask& __b) noexcept {
    return __a = __a | __b;
  }

  friend simd_mask& operator^=(simd_mask& __a, const simd_mask& __b) noexcept {
    return __a = __a ^ __b;
  }

  // simd_mask compares [simd.mask.comparison]
  friend simd_mask operator==(const simd_mask& __a,
                              const simd_mask& __b) noexcept {
    return !(__a ^ __b);
  }

  friend simd_mask operator!=(const simd_mask& __a,
                              const simd_mask& __b) noexcept {
    return __a ^ __b;
  }
};

template <size_t __alignment, class _Tp, class _Abi, class _Up>
void __mask_copy_to(const simd<_Tp, _Abi>& __v, const simd_mask<_Tp, _Abi>& __m,
                    _Up* __buffer __attribute__((align_value(__alignment)))) {
  // TODO: optimize based on bool's bit pattern.
  for (size_t __i = 0; __i < __v.size(); __i++) {
    if (__m[__i]) {
      __buffer[__i] = static_cast<_Up>(__v[__i]);
    }
  }
}

template <size_t __alignment, class _Tp, class _Abi, class _Up>
void __mask_copy_to(const simd_mask<_Tp, _Abi>& __v,
                    const simd_mask<_Tp, _Abi>& __m,
                    _Up* __buffer __attribute__((align_value(__alignment)))) {
  // TODO: optimize based on bool's bit pattern.
  for (size_t __i = 0; __i < __v.size(); __i++) {
    if (__m[__i]) {
      __buffer[__i] = static_cast<_Up>(__v[__i]);
    }
  }
}

template <size_t __alignment, class _Tp, class _Up>
void __mask_copy_to(_Tp __val, bool __m,
                    _Up* __buffer __attribute__((align_value(__alignment)))) {
  if (__m) {
    *__buffer = static_cast<_Up>(__val);
  }
}

template <size_t __alignment, class _Tp, class _Abi, class _Up>
void __mask_copy_from(simd<_Tp, _Abi>& __v, const simd_mask<_Tp, _Abi>& __m,
                      const _Up* __buffer
                      __attribute__((align_value(__alignment)))) {
  // TODO: optimize based on bool's bit pattern.
  for (size_t __i = 0; __i < __v.size(); __i++) {
    if (__m[__i]) {
      __v[__i] = static_cast<_Tp>(__buffer[__i]);
    }
  }
}

template <size_t __alignment, class _Tp, class _Abi, class _Up>
void __mask_copy_from(simd_mask<_Tp, _Abi>& __v,
                      const simd_mask<_Tp, _Abi>& __m,
                      const _Up* __buffer
                      __attribute__((align_value(__alignment)))) {
  // TODO: optimize based on bool's bit pattern.
  for (size_t __i = 0; __i < __v.size(); __i++) {
    if (__m[__i]) {
      __v[__i] = static_cast<bool>(__buffer[__i]);
    }
  }
}

template <size_t __alignment, class _Tp, class _Up>
void __mask_copy_from(_Tp& __val, bool __m,
                      const _Up* __buffer
                      __attribute__((align_value(__alignment)))) {
  if (__m) {
    __val = static_cast<_Tp>(*__buffer);
  }
}

template <class _ValueType>
struct __simd_value_type_traits {
  static_assert(std::is_arithmetic<_ValueType>::value, "");
  using type = _ValueType;
};

template <class _Tp, class _Abi>
struct __simd_value_type_traits<simd<_Tp, _Abi>> {
  static_assert(std::is_arithmetic<_Tp>::value, "");
  using type = _Tp;
};

template <class _Tp, class _Abi>
struct __simd_value_type_traits<simd_mask<_Tp, _Abi>> {
  static_assert(std::is_arithmetic<_Tp>::value, "");
  using type = _Tp;
};

// [simd.whereexpr]
template <class _MaskType, class _ValueType>
class const_where_expression {
  static_assert(std::is_arithmetic<_ValueType>::value ||
                    is_simd<_ValueType>::value ||
                    is_simd_mask<_ValueType>::value,
                "");

  using _Tp = typename __simd_value_type_traits<_ValueType>::type;

  const _MaskType __m_;
  const _ValueType& __v_;

  const_where_expression(const _MaskType& __m, const _ValueType& __v)
      : __m_(__m), __v_(__v) {}

  const_where_expression(const const_where_expression&) = default;

  template <class, class>
  friend class where_expression;

  template <class _Up, class _Ap>
  friend const_where_expression<simd_mask<_Up, _Ap>, simd<_Up, _Ap>>
  where(const typename simd<_Up, _Ap>::mask_type& __m,
        const simd<_Up, _Ap>& __v) noexcept;

  template <class _Up, class _Ap>
  friend const_where_expression<simd_mask<_Up, _Ap>, simd_mask<_Up, _Ap>>
  where(const typename __nodeduce<simd_mask<_Up, _Ap>>::type& __m,
        const simd_mask<_Up, _Ap>& __v) noexcept;

  template <class _Up, class _Mp>
  friend typename std::enable_if<std::is_same<_Mp, bool>::value,
                                 const_where_expression<bool, _Up>>::type
  where(_Mp __m, const _Up& __v) noexcept;

  template <class _Mp, class _Vp, class _BinaryOp>
  friend typename _Vp::value_type
  reduce(const const_where_expression<_Mp, _Vp>& __w,
         typename _Vp::value_type __identity, _BinaryOp __op);

  template <class _Mp, class _Vp>
  friend typename _Vp::value_type
  hmin(const const_where_expression<_Mp, _Vp>& __w);

  template <class _Mp, class _Vp>
  friend typename _Vp::value_type
  hmax(const const_where_expression<_Mp, _Vp>& __w);

public:
  const_where_expression& operator=(const const_where_expression&) = delete;

  _ValueType operator-() const&& {
    return __simd_mask_friend::__simd_select(__v_, _ValueType(0), __m_) -
           __simd_mask_friend::__simd_select(_ValueType(0), __v_, __m_);
  }

  template <class _Up, class _Flags>
  typename std::enable_if<(std::is_same<_Tp, _Up>::value ||
                           !std::is_same<_Tp, bool>::value) &&
                          is_simd_flag_type<_Flags>::value>::type
  copy_to(_Up* __buffer, _Flags) const&& {
    __mask_copy_to<__memory_alignment_impl<_ValueType, _Up, _Flags>::value>(
        __v_, __m_, __buffer);
  }
};

template <class _MaskType, class _ValueType>
class where_expression : public const_where_expression<_MaskType, _ValueType> {
  using _Tp = typename __simd_value_type_traits<_ValueType>::type;

  where_expression(const _MaskType& __m, _ValueType& __v)
      : const_where_expression<_MaskType, _ValueType>(__m, __v) {}

  where_expression(const where_expression&) = default;

  template <class _Up, class _Ap>
  friend where_expression<simd_mask<_Up, _Ap>, simd<_Up, _Ap>>
  where(const typename simd<_Up, _Ap>::mask_type& __m,
        simd<_Up, _Ap>& __v) noexcept;

  template <class _Up, class _Ap>
  friend where_expression<simd_mask<_Up, _Ap>, simd_mask<_Up, _Ap>>
  where(const typename __nodeduce<simd_mask<_Up, _Ap>>::type& __m,
        simd_mask<_Up, _Ap>& __v) noexcept;

  template <class _Up, class _Mp>
  friend typename std::enable_if<std::is_same<_Mp, bool>::value,
                                 where_expression<bool, _Up>>::type
  where(_Mp __m, _Up& __v) noexcept;

  _ValueType& __ref() { return const_cast<_ValueType&>(this->__v_); }

public:
  where_expression& operator=(const where_expression&) = delete;

  template <class _Up>
  auto operator=(_Up&& __u)
      -> decltype(this->__ref() = std::forward<_Up>(__u), void()) {
    this->__ref() = __simd_mask_friend::__simd_select(
        this->__ref(), _ValueType(std::forward<_Up>(__u)), this->__m_);
  }

  template <class _Up>
  auto operator+=(_Up&& __u)
      -> decltype(this->__ref() + std::forward<_Up>(__u), void()) {
    *this = this->__ref() + std::forward<_Up>(__u);
  }

  template <class _Up>
  auto operator-=(_Up&& __u)
      -> decltype(this->__ref() - std::forward<_Up>(__u), void()) {
    *this = this->__ref() - std::forward<_Up>(__u);
  }

  template <class _Up>
  auto operator*=(_Up&& __u)
      -> decltype(this->__ref() * std::forward<_Up>(__u), void()) {
    *this = this->__ref() * std::forward<_Up>(__u);
  }

  template <class _Up>
  auto operator/=(_Up&& __u)
      -> decltype(this->__ref() / std::forward<_Up>(__u), void()) {
    *this = this->__ref() / std::forward<_Up>(__u);
  }

  template <class _Up>
  auto operator%=(_Up&& __u)
      -> decltype(this->__ref() % std::forward<_Up>(__u), void()) {
    *this = this->__ref() % std::forward<_Up>(__u);
  }

  template <class _Up>
  auto operator&=(_Up&& __u)
      -> decltype(this->__ref() & std::forward<_Up>(__u), void()) {
    *this = this->__ref() & std::forward<_Up>(__u);
  }

  template <class _Up>
  auto operator|=(_Up&& __u)
      -> decltype(this->__ref() | std::forward<_Up>(__u), void()) {
    *this = this->__ref() | std::forward<_Up>(__u);
  }

  template <class _Up>
  auto operator^=(_Up&& __u)
      -> decltype(this->__ref() ^ std::forward<_Up>(__u), void()) {
    *this = this->__ref() ^ std::forward<_Up>(__u);
  }

  template <class _Up>
  auto operator<<=(_Up&& __u)
      -> decltype(this->__ref() << std::forward<_Up>(__u), void()) {
    *this = this->__ref() << std::forward<_Up>(__u);
  }

  template <class _Up>
  auto operator>>=(_Up&& __u)
      -> decltype(this->__ref() >> std::forward<_Up>(__u), void()) {
    *this = this->__ref() >> std::forward<_Up>(__u);
  }

  void operator++() { *this += _ValueType(1); }

  void operator++(int) { ++*this; }

  void operator--() { *this -= _ValueType(1); }

  void operator--(int) { --*this; }

  template <class _Up, class _Flags>
  typename std::enable_if<std::is_same<_Tp, _Up>::value ||
                          !std::is_same<_Tp, bool>::value>::type
  copy_from(const _Up* __buffer, _Flags) {
    __mask_copy_from<__memory_alignment_impl<_ValueType, _Up, _Flags>::value>(
        this->__ref(), this->__m_, __buffer);
  }
};

template <class _Tp, class _Abi>
where_expression<simd_mask<_Tp, _Abi>, simd<_Tp, _Abi>>
where(const typename simd<_Tp, _Abi>::mask_type& __m,
      simd<_Tp, _Abi>& __v) noexcept {
  return where_expression<simd_mask<_Tp, _Abi>, simd<_Tp, _Abi>>(__m, __v);
}

template <class _Tp, class _Abi>
const_where_expression<simd_mask<_Tp, _Abi>, simd<_Tp, _Abi>>
where(const typename simd<_Tp, _Abi>::mask_type& __m,
      const simd<_Tp, _Abi>& __v) noexcept {
  return const_where_expression<simd_mask<_Tp, _Abi>, simd<_Tp, _Abi>>(__m,
                                                                       __v);
}

template <class _Tp, class _Abi>
where_expression<simd_mask<_Tp, _Abi>, simd_mask<_Tp, _Abi>>
where(const typename __nodeduce<simd_mask<_Tp, _Abi>>::type& __m,
      simd_mask<_Tp, _Abi>& __v) noexcept {
  return where_expression<simd_mask<_Tp, _Abi>, simd_mask<_Tp, _Abi>>(__m, __v);
}

template <class _Tp, class _Abi>
const_where_expression<simd_mask<_Tp, _Abi>, simd_mask<_Tp, _Abi>>
where(const typename __nodeduce<simd_mask<_Tp, _Abi>>::type& __m,
      const simd_mask<_Tp, _Abi>& __v) noexcept {
  return const_where_expression<simd_mask<_Tp, _Abi>, simd_mask<_Tp, _Abi>>(
      __m, __v);
}

template <class _Tp, class _MaskType>
typename std::enable_if<std::is_same<_MaskType, bool>::value,
                        where_expression<bool, _Tp>>::type
where(_MaskType __m, _Tp& __v) noexcept {
  return where_expression<bool, _Tp>(__m, __v);
}

template <class _Tp, class _MaskType>
typename std::enable_if<std::is_same<_MaskType, bool>::value,
                        const_where_expression<bool, _Tp>>::type
where(_MaskType __m, const _Tp& __v) noexcept {
  return const_where_expression<bool, _Tp>(__m, __v);
}

template <class _MaskType, class _SimdType, class _BinaryOp>
typename _SimdType::value_type
reduce(const const_where_expression<_MaskType, _SimdType>& __w,
       typename _SimdType::value_type __identity, _BinaryOp __op) {
  auto __v = __w.__v_;
  where(!__w.__m_, __v) = _SimdType(__identity);
  return __reduce(__v, __op);
}

template <class _MaskType, class _SimdType>
typename _SimdType::value_type
reduce(const const_where_expression<_MaskType, _SimdType>& __w,
       __simd_plus_op __op = {}) {
  return reduce(__w, typename _SimdType::value_type(0), __op);
}

#if _DIMSUM_STD_VER > 11

template <class _MaskType, class _SimdType>
typename _SimdType::value_type
reduce(const const_where_expression<_MaskType, _SimdType>& __w,
       multiplies<> __op) {
  return reduce(__w, typename _SimdType::value_type(1), __op);
}

template <class _MaskType, class _SimdType>
typename _SimdType::value_type
reduce(const const_where_expression<_MaskType, _SimdType>& __w,
       bit_and<> __op) {
  return reduce(__w, typename _SimdType::value_type(-1), __op);
}

template <class _MaskType, class _SimdType>
typename _SimdType::value_type
reduce(const const_where_expression<_MaskType, _SimdType>& __w, bit_or<> __op) {
  return reduce(__w, typename _SimdType::value_type(0), __op);
}

template <class _MaskType, class _SimdType>
typename _SimdType::value_type
reduce(const const_where_expression<_MaskType, _SimdType>& __w,
       bit_xor<> __op) {
  return reduce(__w, typename _SimdType::value_type(0), __op);
}

#endif // _DIMSUM_STD_VER > 11

template <class _MaskType, class _SimdType>
typename _SimdType::value_type
hmin(const const_where_expression<_MaskType, _SimdType>& __w) {
  return __hmin(__simd_mask_friend::__simd_select(
      _SimdType(std::numeric_limits<typename _SimdType::value_type>::max()),
      __w.__v_, __w.__m_));
}

template <class _MaskType, class _SimdType>
typename _SimdType::value_type
hmax(const const_where_expression<_MaskType, _SimdType>& __w) {
  return __hmax(__simd_mask_friend::__simd_select(
      _SimdType(std::numeric_limits<typename _SimdType::value_type>::lowest()),
      __w.__v_, __w.__m_));
}

// TODO: Implement <cmath> functions for simd.

_DIMSUM_END_NAMESPACE_EXPERIMENTAL_SIMD

#undef _DIMSUM_UNROLL

#endif /* _LIBCPP_EXPERIMENTAL_SIMD */
